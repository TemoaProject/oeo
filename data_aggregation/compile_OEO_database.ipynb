{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import sqlite3\n",
    "import shutil\n",
    "import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "from itertools import combinations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "single_region = 0\n",
    "timeslice = 0\n",
    "elec_only = 0\n",
    "remove_elec = 0\n",
    "\n",
    "if single_region ==1:\n",
    "    file_prefix = '../../PowerGenome/OEO_national/OEO_national_settings_'\n",
    "else:\n",
    "    if elec_only==0:\n",
    "        file_prefix = '../../PowerGenome/OEO_regional/OEO_regional_settings_'\n",
    "    else:\n",
    "        file_prefix = '../../PowerGenome/OEO_regional_elec/OEO_regional_settings_elec_'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create New SQL File\n",
    "emptydB = 'US_BASE.sqlite'\n",
    "#outfilename_w_ext = outFilename + '.sqlite'\n",
    "if (single_region==1) & (timeslice==1) :\n",
    "    outputdB = '../US_1R_TS.sqlite'\n",
    "elif (single_region==1) & (timeslice==0) :\n",
    "    outputdB = '../US_1R_4D.sqlite'\n",
    "elif elec_only ==1:\n",
    "    outputdB = '../US_9R_24D_elec.sqlite'\n",
    "elif remove_elec ==1:\n",
    "    outputdB = 'US_Regional_noelec.sqlite'\n",
    "elif (single_region==0) & (timeslice==1):\n",
    "    outputdB = '../US_9R_TS.sqlite'\n",
    "else:\n",
    "    outputdB = '../US_9R_8D.sqlite'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_periods = list(np.arange(2020,2051,5))\n",
    "start_year = all_periods[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if elec_only==1:\n",
    "#     elec_demand = pd.DataFrame()\n",
    "#     for year in all_periods:\n",
    "#         load_curves_year = make_final_load_curves(pudl_engine, scenario_settings[year][scenario])\n",
    "#         load_curves_year.loc[:,'periods']=year\n",
    "#         elec_demand = pd.concat([elec_demand, load_curves_year])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#based on NREL ReEDS documentation, 2020: https://www.nrel.gov/docs/fy21osti/78195.pdf, Table 10 onwards\n",
    "generator_lifetimes = dict()\n",
    "# generator_lifetimes['BECCS'] = 50\n",
    "generator_lifetimes['BIO'] = 50\n",
    "generator_lifetimes['Batt'] = 15\n",
    "generator_lifetimes['COALIGCC'] = 75\n",
    "generator_lifetimes['COALSTM'] = 75\n",
    "generator_lifetimes['COALUSC'] = 75\n",
    "generator_lifetimes['GEO'] = 30\n",
    "generator_lifetimes['H2'] = 60\n",
    "generator_lifetimes['HYD'] = 100\n",
    "generator_lifetimes['E_NGAACT'] = 50\n",
    "generator_lifetimes['E_NGACT'] = 50\n",
    "generator_lifetimes['E_NGAACC'] = 60\n",
    "generator_lifetimes['E_NGACC'] = 60\n",
    "generator_lifetimes['E_NGASTM'] = 75\n",
    "generator_lifetimes['WND'] = 30 \n",
    "generator_lifetimes['SOL'] = 30\n",
    "generator_lifetimes['TRANS'] = 60\n",
    "generator_lifetimes['URN'] = 70"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_gens = pd.read_csv(file_prefix + 'all_gens.csv')\n",
    "new_gen = pd.read_csv(file_prefix + 'new_gen.csv')\n",
    "\n",
    "gen_variability = pd.read_csv(file_prefix + 'gen_variability.csv')\n",
    "\n",
    "load_curves = pd.read_csv(file_prefix + 'load_curves.csv')\n",
    "if '_4D' in outputdB:\n",
    "    reduced_load_profile = pd.read_csv(file_prefix + 'reduced_load_profile_2P_2D.csv')\n",
    "    time_series_mapping = pd.read_csv(file_prefix + 'time_series_mapping_2P_2D.csv')\n",
    "    reduced_resource_profile = pd.read_csv(file_prefix + 'reduced_resource_2P_2D.csv')\n",
    "elif '_24D' in outputdB:\n",
    "    reduced_load_profile = pd.read_csv(file_prefix + 'reduced_load_profile_4P_6D.csv')\n",
    "    time_series_mapping = pd.read_csv(file_prefix + 'time_series_mapping_4P_6D.csv')\n",
    "    reduced_resource_profile = pd.read_csv(file_prefix + 'reduced_resource_4P_6D.csv')\n",
    "elif '_21D' in outputdB:\n",
    "    reduced_load_profile = pd.read_csv(file_prefix + 'reduced_load_profile_3P_7D.csv')\n",
    "    time_series_mapping = pd.read_csv(file_prefix + 'time_series_mapping_3P_7D.csv')\n",
    "    reduced_resource_profile = pd.read_csv(file_prefix + 'reduced_resource_3P_7D.csv')\n",
    "elif '_14D' in outputdB:\n",
    "    reduced_load_profile = pd.read_csv(file_prefix + 'reduced_load_profile_2P_7D.csv')\n",
    "    time_series_mapping = pd.read_csv(file_prefix + 'time_series_mapping_2P_7D.csv')\n",
    "    reduced_resource_profile = pd.read_csv(file_prefix + 'reduced_resource_2P_7D.csv')\n",
    "elif '_12D' in outputdB:\n",
    "    reduced_load_profile = pd.read_csv(file_prefix + 'reduced_load_profile_3P_4D.csv')\n",
    "    time_series_mapping = pd.read_csv(file_prefix + 'time_series_mapping_3P_4D.csv')\n",
    "    reduced_resource_profile = pd.read_csv(file_prefix + 'reduced_resource_3P_4D.csv')\n",
    "elif '_8D' in outputdB:\n",
    "    reduced_load_profile = pd.read_csv(file_prefix + 'reduced_load_profile_2P_4D.csv')\n",
    "    time_series_mapping = pd.read_csv(file_prefix + 'time_series_mapping_2P_4D.csv')\n",
    "    reduced_resource_profile = pd.read_csv(file_prefix + 'reduced_resource_2P_4D.csv')\n",
    "\n",
    "if single_region!=1:\n",
    "    if os.path.exists(file_prefix + 'transmission.csv'):\n",
    "        transmission = pd.read_csv(file_prefix + 'transmission.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "map_tech_names = dict()\n",
    "map_tech_names['biomass'] = 'E_BIO_R'\n",
    "map_tech_names['conventional_hydroelectric'] = 'E_HYDCONV_R'\n",
    "map_tech_names['conventional_steam_coal'] = 'E_COALSTM_N'\n",
    "map_tech_names['natural_gas_fired_combined_cycle'] = 'E_NGACC_R'\n",
    "map_tech_names['natural_gas_fired_combustion_turbine'] = 'E_NGACT_R'\n",
    "map_tech_names['natural_gas_steam_turbine'] = 'E_NGASTM_R'\n",
    "map_tech_names['nuclear'] = 'E_URNLWR_R'\n",
    "map_tech_names['onshore_wind_turbine'] = 'E_WND_R'\n",
    "map_tech_names['small_hydroelectric'] = 'E_HYDSM_R'\n",
    "map_tech_names['solar_photovoltaic'] = 'E_SOLPV_R'\n",
    "map_tech_names['hydroelectric_pumped_storage'] = 'E_HYDPS_R'\n",
    "map_tech_names['geothermal'] = 'E_GEO_R'\n",
    "map_tech_names['naturalgas_ccccsavgcf_conservative'] = 'E_NGACC_CCS_N'\n",
    "map_tech_names['naturalgas_ccavgcf_moderate'] = 'E_NGAACC_N'\n",
    "map_tech_names['naturalgas_ctavgcf_moderate'] = 'E_NGAACT_N'\n",
    "map_tech_names['landbasedwind_class4_moderate'] = 'E_WND_N'\n",
    "map_tech_names['utilitypv_class1_moderate'] = 'E_SOLPVCEN_N'\n",
    "map_tech_names['naturalgas_ccs100_moderate'] = 'E_NGACC_CCS_ZERO_N'\n",
    "map_tech_names['nuclear_mid'] = 'E_URNLWR_N'\n",
    "map_tech_names['battery_moderate'] = 'E_Batt'\n",
    "map_tech_names['ev_load_shifting'] = 'E_LD_SHFT'\n",
    "map_tech_names['offshorewind_class10_moderate'] = 'E_OFWND_N'\n",
    "map_tech_names['geothermal_hydroflash'] = 'E_GEOF_N'\n",
    "map_tech_names['geothermal_hydrobinary'] = 'E_GEOB_N'\n",
    "map_tech_names['csp_class5_10hourstes'] = 'E_SOLTHCEN_N'\n",
    "map_tech_names['biopower_dedicated_moderate'] = 'E_BIO_N'\n",
    "map_tech_names['coal_igccavgcf_moderate'] = 'E_COALIGCC_N'\n",
    "map_tech_names['coal_ccs90avgcf_moderate'] = 'E_COALUSC_90CCS_N'\n",
    "map_tech_names['respv_class3'] = 'E_SOLPVENDUSE_N'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "map_tech_desc = dict()\n",
    "map_tech_desc['E_BIO_R'] = '#existing bio-energy'\n",
    "map_tech_desc['E_BIO_N'] = '#new bio-energy'\n",
    "map_tech_desc['E_HYDSM_R'] = '#existing small hydroelectric power plant'\n",
    "map_tech_desc['E_HYDPS_R'] = '#existing pumped hydro storage'\n",
    "map_tech_desc['E_WND_N'] = '#new wind power plant'\n",
    "map_tech_desc['E_NGACC_CCS_ZERO_N'] = '#new natural gas combined cycle with 100% CCS power plant'\n",
    "map_tech_desc['E_OFWND_N'] = '#new offshore wind, floating'\n",
    "map_tech_desc['E_GEOF_N'] = '#new geothermal, hydro flash'\n",
    "map_tech_desc['E_GEOB_N'] = '#new geothermal, hydro binary'\n",
    "map_tech_desc['E_COALUSC_90CCS_N'] = '#new ultrasupercritical pulverized coal with 90% CCS power plant'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#map PG technology names to OEO names\n",
    "all_gens['Resource'] = all_gens['Resource'].str.split('_').map(lambda x: '_'.join(x[0:3] if x[-1].isnumeric() else x))\n",
    "new_gen['Resource'] = new_gen['Resource'].str.split('_').map(lambda x: '_'.join(x[0:3] if x[-1].isnumeric() else x))\n",
    "all_gens['Resource'] = all_gens['Resource'].map(map_tech_names)\n",
    "new_gen['Resource'] = new_gen['Resource'].map(map_tech_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#multiply capex values by the regional multipliers, since this is not done in PowerGenome\n",
    "\n",
    "new_gen['capex_mw'] *= new_gen['regional_cost_multiplier']\n",
    "all_gens['capex_mw'] *= all_gens['regional_cost_multiplier']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "include_co2_trans = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conn = sqlite3.connect(emptydB)\n",
    "c = conn.cursor()\n",
    "tech_table = pd.read_sql_query(\"SELECT * FROM technologies\", conn)\n",
    "tech_table.rename(columns={'tech':'tech_technologies'}, inplace=True)\n",
    "\n",
    "input_table = pd.read_sql_query(\"SELECT input_comm, tech FROM Efficiency WHERE tech in \\\n",
    "(SELECT tech FROM technologies where sector='electric')\", conn)\n",
    "input_table = {k: list(v) for k,v in input_table.drop_duplicates().groupby('tech')[\"input_comm\"]}\n",
    "conn.close()\n",
    "\n",
    "\n",
    "#input comms for technologies not specified in US_National\n",
    "input_table['E_BIO_R'] = ['HERB_BIO']\n",
    "input_table['E_BIO_N'] = input_table['E_BIOIGCC_N'] \n",
    "input_table['E_HYDPS_R'] = ['ELC']\n",
    "input_table['E_HYDSM_R'] = ['ethos_R']\n",
    "input_table['E_COALUSC_90CCS_N'] = ['COALIGCC_N'] #COALSTM_N\n",
    "input_table['E_COALIGCC_N'] = ['COALIGCC_N']#COALSTM_N\n",
    "input_table['E_NGACC_CCS_ZERO_N'] = ['E_NGA']\n",
    "input_table['E_GEOB_N'] = ['ethos_R']\n",
    "input_table['E_GEOF_N'] = ['ethos_R']\n",
    "input_table['E_WND_N'] = ['ethos_R']\n",
    "input_table['E_OFWND_N'] = ['ethos_R']\n",
    "input_table['E_TRANS_R'] = ['ELC']\n",
    "input_table['E_TRANS_N'] = ['ELC']\n",
    "if include_co2_trans==1:\n",
    "    input_table['co2_TRANS_N'] = ['co2_to_ground']\n",
    "\n",
    "#emissionsactivity for technologies not specified in US_National \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_count = all_gens[['technology', 'cluster']].groupby('technology').max().reset_index()\n",
    "cluster_count.columns = ['technology', 'max_cluster']\n",
    "all_gens = all_gens.merge(cluster_count, on='technology', how='left')\n",
    "\n",
    "mask = all_gens['max_cluster']<=1\n",
    "all_gens.loc[mask, 'tech'] =  all_gens.loc[mask, 'Resource'] + '-' + all_gens.loc[mask,'region']\n",
    "all_gens.loc[~mask, 'tech'] =  all_gens.loc[~mask, 'Resource'] + '-' + all_gens.loc[~mask,'region'] + \\\n",
    "'-' +all_gens.loc[~mask,'cluster'].map(int).map(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_count = new_gen[['technology', 'cluster']].groupby('technology').max().reset_index()\n",
    "cluster_count.columns = ['technology', 'max_cluster']\n",
    "new_gen = new_gen.merge(cluster_count, on='technology', how='left')\n",
    "\n",
    "mask = new_gen['max_cluster']<=1\n",
    "new_gen.loc[mask, 'tech'] =  new_gen.loc[mask, 'Resource'] + '-' + new_gen.loc[mask,'region']\n",
    "new_gen.loc[~mask, 'tech'] =  new_gen.loc[~mask, 'Resource'] + '-' + new_gen.loc[~mask,'region'] + \\\n",
    "'-' +new_gen.loc[~mask,'cluster'].map(int).map(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#concat start year gens with remaining new gens\n",
    "all_gens_multi_year = pd.concat([all_gens, new_gen])\n",
    "\n",
    "all_gens_multi_year = all_gens_multi_year.merge(tech_table, left_on = ['Resource'], right_on =['tech_technologies'], how='left')\n",
    "all_gens_multi_year['flag'].fillna('p', inplace=True)\n",
    "all_gens_multi_year['sector'].fillna('electric', inplace=True)\n",
    "all_gens_multi_year['tech_desc'].fillna(all_gens_multi_year['Resource'].map(map_tech_desc), inplace=True)\n",
    "\n",
    "all_gens_multi_year.loc[all_gens_multi_year['Resource']=='E_HYDPS_R','flag'] = 'ps'\n",
    "\n",
    "#all_gens_multi_year = all_gens_multi_year[all_gens.columns]\n",
    "all_gens_multi_year.loc[np.isnan(all_gens_multi_year.operating_year),'operating_year'] = all_periods[0]\n",
    "all_gens_multi_year.loc[all_gens_multi_year.operating_year==0,'operating_year'] = all_periods[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#account for storage (batteries) capex and FOM for a specific storage duration\n",
    "stor_dur = 4\n",
    "mask = all_gens_multi_year['tech'].str.contains('Batt')\n",
    "all_gens_multi_year.loc[mask, 'Heat_Rate_MMBTU_per_MWh'] = 3.412/0.85 #assuming 85% efficiency\n",
    "orig_batt = all_gens_multi_year.loc[mask,:].copy()\n",
    "all_gens_multi_year.loc[mask,'capex_mw'] += stor_dur*all_gens_multi_year.loc[mask,'capex_mwh']\n",
    "all_gens_multi_year.loc[mask,'Fixed_OM_Cost_per_MWyr'] += stor_dur*all_gens_multi_year.loc[mask,'Fixed_OM_Cost_per_MWhyr']\n",
    "#all_gens_multi_year.loc[mask,'Fixed_OM_Cost_per_MWyr'] = 2.5/100*all_gens_multi_year.loc[mask,'capex_mwh']\n",
    "stor_dur = 8\n",
    "orig_batt.loc[mask,'capex_mw'] += stor_dur*orig_batt.loc[mask,'capex_mwh']\n",
    "orig_batt.loc[mask,'Fixed_OM_Cost_per_MWyr'] += stor_dur*orig_batt.loc[mask,'Fixed_OM_Cost_per_MWhyr']\n",
    "#orig_batt.loc[mask,'Fixed_OM_Cost_per_MWyr'] = 2.5/100*orig_batt.loc[mask,'capex_mwh']\n",
    "orig_batt['Resource'] = 'E_Batt8hr'\n",
    "orig_batt['tech'] = orig_batt['tech'].str.replace('E_Batt','E_Batt8hr')\n",
    "orig_batt['tech_technologies'] = orig_batt['tech_technologies'].str.replace('E_Batt','E_Batt8hr')\n",
    "orig_batt['tech_desc'] = orig_batt['tech_desc'].str.replace('storage','storage (8 hour)')\n",
    "\n",
    "all_gens_multi_year = pd.concat([all_gens_multi_year, orig_batt])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#duplicate NGCC costs, efficiencies to represent H2CC\n",
    "mask = all_gens_multi_year['tech'].str.contains('E_NGAACC_N')\n",
    "orig_ngcc = all_gens_multi_year.loc[mask,:].copy()\n",
    "orig_ngcc.loc[:,'tech_desc'] = '#hydrogen combustion in a combined cycle plants for electricity generation'\n",
    "orig_ngcc.loc[:, 'tech_technologies'] = orig_ngcc.loc[:, 'tech_technologies'].str.replace('NGAA', 'H2')\n",
    "orig_ngcc.loc[:, 'tech'] = orig_ngcc.loc[:, 'tech'].str.replace('NGAA', 'H2')\n",
    "orig_ngcc.loc[:, 'Resource'] = 'H2_100'\n",
    "\n",
    "all_gens_multi_year = pd.concat([all_gens_multi_year, orig_ngcc])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create capacity factor dataframe\n",
    "if timeslice==1:\n",
    "    df_capfac_all= gen_variability.reset_index(drop=True)\n",
    "    df_capfac_all.columns = all_gens.loc[:,'tech'] #rename columns to match df_gen technologies\n",
    "    df_capfac_all.loc[:,'hour'] = np.tile(np.arange(0,24),365)\n",
    "    df_capfac_all.loc[((df_capfac_all['hour']>=6) & (df_capfac_all['hour']<12)), 'time_of_day_name'] = 'am'\n",
    "    df_capfac_all.loc[((df_capfac_all['hour']>=12) & (df_capfac_all['hour']<15)), 'time_of_day_name'] = 'peak'\n",
    "    df_capfac_all.loc[((df_capfac_all['hour']>=15) & (df_capfac_all['hour']<21)), 'time_of_day_name'] = 'pm'\n",
    "    df_capfac_all.loc[((df_capfac_all['hour']>=21) | (df_capfac_all['hour']<6)), 'time_of_day_name'] = 'night'\n",
    "    summer = np.arange(171*24, 265*24) #June 20 to Sep 22\n",
    "    winter = np.concatenate((np.arange(0,79*24), np.arange(355*24,8760))) #dec 21st#march 20th \n",
    "    df_capfac_all.loc[:, 'season_name'] = 'Intermediate'\n",
    "    df_capfac_all.loc[summer, 'season_name'] = 'Summer'\n",
    "    df_capfac_all.loc[winter, 'season_name'] = 'Winter'\n",
    "    df_capfac = df_capfac_all.groupby(['time_of_day_name', 'season_name']).mean().reset_index()\n",
    "    df_capfac.drop(columns=['hour'], inplace=True)\n",
    "else:\n",
    "    df_capfac= reduced_resource_profile.reset_index(drop=True)\n",
    "    df_capfac.columns = all_gens.loc[:,'tech'] #rename columns to match df_gen technologies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove solar thermal from all regions except CA and SW\n",
    "#gens\n",
    "mask = (all_gens_multi_year['Resource'].str.contains('E_SOLTH')) & \\\n",
    "((~all_gens_multi_year['tech'].str.contains('-CA')) & (~all_gens_multi_year['tech'].str.contains('-SW')))\n",
    "all_gens_multi_year = all_gens_multi_year.loc[~mask,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def return_region(df_tech):\n",
    "    return [x[1] for x in df_tech.str.split('-')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def return_tech(df_tech):\n",
    "    output_vals = []\n",
    "    for val in df_tech:\n",
    "        try:\n",
    "            o_val = val.split('-')[0] + '_' + val.split('-')[2]\n",
    "        except:\n",
    "            o_val = val.split('-')[0]\n",
    "        output_vals.append(o_val)\n",
    "    return output_vals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reg_mult = dict()\n",
    "reg_mult['E_BECCS_N'] = new_gen.loc[new_gen['Resource']=='E_COALUSC_90CCS_N', ['region','regional_cost_multiplier']].drop_duplicates().set_index('region')\n",
    "reg_mult['H2_STO150'] = new_gen.loc[new_gen['Resource']=='E_Batt', ['region','regional_cost_multiplier']].drop_duplicates().set_index('region')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "regions_list = all_gens_multi_year.region.unique()\n",
    "\n",
    "# Delete old *.sqlite file (if it already exists) and copy/rename copy of temoa_schema.sqlite\n",
    "if os.path.isfile(outputdB):\n",
    "    os.remove(outputdB)\n",
    "shutil.copyfile(emptydB, outputdB)\n",
    "##remove data from tables\n",
    "conn = sqlite3.connect(outputdB)\n",
    "c = conn.cursor()\n",
    "\n",
    "table_list = c.execute(\"SELECT name FROM sqlite_master WHERE type='table' AND name NOT LIKE 'Output%'\").fetchall()\n",
    "c.execute(\"UPDATE Efficiency SET tech = TRIM(tech);\") #trim spaces. Need to trim carriage return\n",
    "\n",
    "#remove entire table\n",
    "for table in [#'LifetimeProcess', #'CapacityToActivity'\n",
    "              'CapacityCredit', 'CapacityFactorTech', 'MinGenGroupWeight','MinGenGroupTarget', 'MinCapacity',#'StorageDuration',\n",
    "              #'GrowthRateMax','GrowthRateSeed', #'MinActivity',\n",
    "               'time_periods','tech_curtailment',\n",
    "              'Output_CapacityByPeriodAndTech','Output_V_Capacity','Output_VFlow_In', 'Output_VFlow_Out',\n",
    "              'Output_Objective', 'Output_Emissions', 'Output_curtailment', 'Output_Costs',\n",
    "               'tech_groups', 'groups', 'MyopicBaseyear', 'SegFrac','DemandSpecificDistribution'\n",
    "              ]:\n",
    "    query = \"\"\"DELETE FROM \"\"\" + table\n",
    "    c.execute(query)\n",
    "    \n",
    "if timeslice!=1:\n",
    "    for table in ['time_of_day', 'time_season']:\n",
    "        query = \"\"\"DELETE FROM \"\"\" + table\n",
    "        c.execute(query)\n",
    "        \n",
    "if single_region==0:\n",
    "    query = \"UPDATE EmissionLimit SET regions='global'\"\n",
    "    #query = \"DELETE FROM EmissionLimit\"\n",
    "    c.execute(query)\n",
    "   \n",
    "\n",
    "#delete distributed generation (from solar) from Efficiency table in original database\n",
    "#query = \"\"\"DELETE FROM Efficiency WHERE input_comm='ELCDIST_R'\"\"\"\n",
    "#c.execute(query)\n",
    "query = \"\"\"DELETE FROM Efficiency WHERE tech='IMPELC'\"\"\"\n",
    "c.execute(query)\n",
    "\n",
    "#removing upstream 'clean coal pathway, with pre-combustion retrofits'\n",
    "query = \"\"\"DELETE FROM Efficiency WHERE tech='E_CCR_COALSTM_N'\"\"\"\n",
    "c.execute(query)\n",
    "\n",
    "#deleting variable costs associated with transmission\n",
    "#query = \"\"\"DELETE FROM CostVariable WHERE tech='E_ELCTDLOSS'\"\"\"\n",
    "#c.execute(query)\n",
    "\n",
    "#modify max capacity geo techs\n",
    "tech_keep = 'E_GEOBCFS_N'\n",
    "df = pd.read_sql_query(\"SELECT * FROM MaxCapacity WHERE tech='\" + tech_keep + \"'\", conn)\n",
    "df1 = df.copy()\n",
    "df2 = df.copy()\n",
    "df1['tech'] = 'E_GEOF_N'\n",
    "df2['tech'] = 'E_GEOB_N'\n",
    "query = \"DELETE FROM MaxCapacity WHERE tech='\" + tech_keep + \"'\"\n",
    "c.execute(query)\n",
    "df_new = pd.concat([df1, df2])\n",
    "df_new.to_sql('MaxCapacity',conn, if_exists='append', index=False)\n",
    "\n",
    "\n",
    "techs_keep = ['E_BECCS_N','H2_STO150', 'E_BECCS_N_emissions']#, 'E_H2CC_N']\n",
    "old_df_c2a = pd.read_sql(\"SELECT * FROM CapacityToActivity\", conn)\n",
    "#query = \"DELETE FROM CapacityToActivity WHERE tech NOT IN ('\" + \"','\".join(techs_keep) + \"')\"\n",
    "#c.execute(query)\n",
    "for tech_keep in techs_keep:\n",
    "    #modify the technologies that are retained from US_National for multiple regions\n",
    "    for table in table_list:\n",
    "        df_cols = c.execute(\"SELECT * FROM pragma_table_info('\" + table[0] + \"')\").fetchall()\n",
    "        df_cols = [x[1] for x in df_cols]\n",
    "        if 'tech' in df_cols:\n",
    "            df = pd.read_sql_query(\"SELECT * FROM \" + table[0] + \" WHERE tech='\" + tech_keep + \"'\", conn)\n",
    "            if table[0]=='CapacityFactorTech':\n",
    "                query = \"DELETE FROM \" + table[0] + \" WHERE tech='\" + tech_keep + \"'\"\n",
    "                c.execute(query)\n",
    "            elif (len(df)>0) & ('regions' in df.columns):\n",
    "                query = \"DELETE FROM \" + table[0] + \" WHERE tech='\" + tech_keep + \"'\"\n",
    "                c.execute(query)\n",
    "                for reg in regions_list:\n",
    "                    df_new = df.copy()\n",
    "                    df_new['regions'] = reg\n",
    "                    if table[0]=='CostInvest':\n",
    "                        df_new['cost_invest'] *= reg_mult[tech_keep].loc[reg].values[0]\n",
    "                    df_new.to_sql(table[0],conn, if_exists='append', index=False)\n",
    "        \n",
    "#modify efficiency for processes that meet demands that can only be met by processes of vintage 2017\n",
    "for table in ['Efficiency','EmissionActivity']:\n",
    "    query = \"\"\"UPDATE \"\"\" + table + \"\"\" SET vintage=2020\n",
    "    WHERE vintage=2017\n",
    "    AND regions || input_comm || tech || output_comm NOT IN \n",
    "    (\n",
    "        SELECT DISTINCT\n",
    "        regions || input_comm || tech || output_comm\n",
    "        FROM \"\"\" + table + \"\"\" WHERE vintage = 2020\n",
    "    )\"\"\"\n",
    "    c.execute(query)\n",
    "\n",
    "#modify cost tables to account for these processes\n",
    "for table in ['CostFixed','CostVariable']:\n",
    "    query= \"\"\"UPDATE \"\"\" + table + \"\"\" SET vintage=2020\n",
    "        WHERE \n",
    "          vintage = 2017 \n",
    "          AND\n",
    "          regions || tech NOT IN \n",
    "        (SELECT DISTINCT\n",
    "          regions || tech\n",
    "        FROM \"\"\" + table + \"\"\" WHERE vintage = 2020)\"\"\"\n",
    "    c.execute(query)\n",
    "\n",
    "#delete all elec technologies except for those in techs_keep\n",
    "for table in ['Efficiency', 'ExistingCapacity', 'DiscountRate', 'CostVariable', 'CostFixed', 'CostInvest']:\n",
    "    query = \"DELETE FROM \" + table + \" WHERE tech IN\\\n",
    "    (SELECT tech FROM technologies WHERE sector='electric')\\\n",
    "    AND tech NOT IN ('\" + \"','\".join(techs_keep) + \"')\"\n",
    "    c.execute(query)\n",
    "\n",
    "#delete efficiencies from 2017 vintages\n",
    "for table in ['Efficiency','EmissionActivity']:\n",
    "    query = \"\"\"DELETE FROM \"\"\" + table + \"\"\" WHERE vintage=2017\"\"\"\n",
    "    c.execute(query)\n",
    "\n",
    "\n",
    "table = 'EmissionLimit'\n",
    "query = \"\"\"DELETE FROM \"\"\" + table + \"\"\" WHERE periods=2017\"\"\"\n",
    "c.execute(query)    \n",
    "        \n",
    "# Delete row from Efficiency if (t,v) retires at the begining of crent period (which is time_periods[i][0])\n",
    "c.execute(\"DELETE FROM Efficiency WHERE tech IN (SELECT tech FROM LifetimeProcess WHERE \\\n",
    "             LifetimeProcess.life_process+LifetimeProcess.vintage<=2020) \\\n",
    "             AND vintage IN (SELECT vintage FROM LifetimeProcess WHERE LifetimeProcess.life_process+\\\n",
    "             LifetimeProcess.vintage<=2020);\")\n",
    "\n",
    "# Delete row from Efficiency if (t,v) retires at the begining of crent period (which is time_periods[i][0])\n",
    "c.execute(\"DELETE FROM Efficiency WHERE tech IN (SELECT tech FROM LifetimeTech WHERE \\\n",
    "             LifetimeTech.life+Efficiency.vintage<=2020) AND \\\n",
    "             vintage NOT IN (SELECT vintage FROM LifetimeProcess WHERE LifetimeProcess.tech\\\n",
    "             =Efficiency.tech);\")\n",
    "\n",
    "# If row is not deleted via the last two DELETE commands, it might still be invalid for period\n",
    "#  time_periods[i][0] since they can have model default lifetime of 40 years. \n",
    "c.execute(\"DELETE FROM Efficiency WHERE tech IN (SELECT tech FROM Efficiency WHERE \\\n",
    "            40+Efficiency.vintage<=2020) AND \\\n",
    "            tech NOT IN (SELECT tech FROM LifetimeTech) AND \\\n",
    "            vintage NOT IN (SELECT vintage FROM LifetimeProcess WHERE LifetimeProcess.tech=Efficiency.tech);\")\n",
    "\n",
    "\n",
    "#remove demands, techinput split before 2020\n",
    "for table in ['Demand','TechInputSplit', 'MinActivity', 'MaxActivity','CostFixed','CostVariable']:\n",
    "    query = \"\"\"DELETE FROM \"\"\" + table +\"\"\" where periods < 2020\"\"\"\n",
    "    c.execute(query)\n",
    "for table in ['CostFixed','CostVariable']:\n",
    "    query = \"\"\"DELETE FROM \"\"\" + table +\"\"\" where vintage=2017\"\"\"\n",
    "    c.execute(query)\n",
    "for table in ['CostInvest', 'DiscountRate']:\n",
    "    query = \"\"\"DELETE FROM \"\"\" + table + \"\"\" where vintage < 2020\"\"\"\n",
    "    c.execute(query)\n",
    "    \n",
    "#if creating electricity only database, delete all other demands\n",
    "if elec_only==1:\n",
    "    query = \"DELETE FROM Efficiency WHERE output_comm IN (SELECT demand_comm FROM Demand WHERE demand_comm IS NOT 'dummy_sink')\"\n",
    "    c.execute(query)\n",
    "    c.execute(\"DELETE FROM Demand WHERE demand_comm IS NOT 'dummy_sink'\")\n",
    "    c.execute(\"DELETE FROM DemandSpecificDistribution\")\n",
    "elif timeslice==1: \n",
    "    c.execute(\"DELETE FROM DemandSpecificDistribution\")\n",
    "\n",
    "conn.commit()\n",
    "conn.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def swap_regions(x):\n",
    "    return x.split('-')[1] + '-' + x.split('-')[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove coal and gas clusters with greater than 60% efficiency\n",
    "mask = (all_gens_multi_year['Heat_Rate_MMBTU_per_MWh']<3.412/0.6) & \\\n",
    "((all_gens_multi_year['technology'].str.contains('Natural')) | (all_gens_multi_year['technology'].str.contains('Coal')))\n",
    "all_gens_multi_year = all_gens_multi_year.loc[~mask]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#check data for existing nuclear which is currently zero\n",
    "all_gens_multi_year['Var_OM_Cost_per_MWh'].fillna(0, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#def prep_tables(period, folder_name, gen_data, capfac_variability_data):\n",
    "    #PREPARE DATABASE TABLES\n",
    "    \n",
    "df_gen= all_gens_multi_year\n",
    "df_gen.rename(columns={'region':'regions'}, inplace=True)\n",
    "\n",
    "#remove clusters with cap size of less than 100 MW\n",
    "\n",
    "mask1 = (df_gen['tech'].str.contains('E_WND_N')) | (df_gen['tech'].str.contains('E_SOLPVCEN_N'))\n",
    "mask2 = df_gen['Max_Cap_MW']<100\n",
    "df_capfac = df_capfac.loc[:,~df_capfac.columns.isin(df_gen.loc[mask1 & mask2, 'tech'].drop_duplicates().values)]\n",
    "df_gen = df_gen.loc[~(mask1 & mask2),:]\n",
    "\n",
    "df_gen.loc[:,'regions'] = df_gen.loc[:,'regions'].str.replace('US_N','US')\n",
    "#df_gen.loc[:, 'tech'] =  df_gen.loc[:, 'Resource'] + '-' + df_gen.loc[:,'regions'] + '-' +df_gen.loc[:,'cluster'].map(int).map(str)\n",
    "\n",
    "#remove battery, pumped storage and ev_shifting (after column names have been assigned to capfac dataframe)\n",
    "df_gen = df_gen.loc[~df_gen.loc[:,'tech'].str.contains('E_LD_SHFT'),:]\n",
    "#df_gen = df_gen.loc[~df_gen.loc[:,'tech'].str.contains('battery'),:]\n",
    "#df_gen = df_gen.loc[~df_gen.loc[:,'tech'].str.contains('pumped'),:]\n",
    "#rename operating_year as vintage\n",
    "df_gen.rename(columns={'operating_year':'vintage'},inplace=True)\n",
    "df_gen= df_gen.astype({'vintage':int})\n",
    "\n",
    "#modify lifetimes of clusters post-PG runs\n",
    "df_gen.loc[:,'lifetime'] = [generator_lifetimes[y] for x in df_gen['tech'] for y in generator_lifetimes.keys() if y.lower() in x.lower()] #map lifetimes\n",
    "\n",
    "#capacityfactortech\n",
    "#remove ev_shifting\n",
    "df_capfac.drop(columns=df_capfac.columns[df_capfac.columns.str.contains('E_LD_SHFT')], inplace=True)\n",
    "#df_capfac.drop(columns=df_capfac.columns[df_capfac.columns.str.contains('battery')], inplace=True)\n",
    "#df_capfac.drop(columns=df_capfac.columns[df_capfac.columns.str.contains('pumped')], inplace=True)\n",
    "\n",
    "intra_annual_periods = len(df_capfac)\n",
    "if timeslice==0:\n",
    "    days = len(time_series_mapping['Rep_Period'].unique()) #intra_annual_periods/24\n",
    "    iter_val = 0\n",
    "    hours = int(len(df_capfac)/days)\n",
    "    for day in np.arange(days)+1:\n",
    "        df_capfac.loc[iter_val:iter_val+hours-1,'season_name'] = 'S' + str(int(day))\n",
    "        df_capfac.loc[iter_val:iter_val+hours-1,'time_of_day_name'] = ['H' + str(int(x)) for x in np.arange(1,hours+1)]\n",
    "        iter_val+=hours\n",
    "df_capfac = df_capfac.melt(id_vars=['season_name','time_of_day_name'])\n",
    "df_capfac = df_capfac.rename(columns={'variable':'tech', 'value':'cf_tech'})\n",
    "df_capfac.loc[:,'cf_tech_notes'] = 'from PowerGenome'\n",
    "df_capfac.insert(0,'regions',return_region(df_capfac.loc[:,'tech']))\n",
    "#df_capfac.loc[:,'regions'] = df_capfac.loc[:,'regions'].str.replace('US_N','US')\n",
    "df_capfac.loc[:,'tech'] = return_tech(df_capfac.loc[:,'tech'])\n",
    "    \n",
    "#remove techs that have constant capacity factor\n",
    "df_sum = df_capfac.groupby(by=['regions','tech']).sum().reset_index()\n",
    "df_fixed_capfac = df_sum[df_sum.cf_tech==intra_annual_periods]\n",
    "df_fixed_capfac.rename(columns={'cf_tech': 'cf_tech_max'}, inplace=True)\n",
    "df_capfac = df_capfac.merge(df_fixed_capfac, on = ['regions','tech'], how='outer')\n",
    "df_capfac = df_capfac[df_capfac.cf_tech_max!=intra_annual_periods]\n",
    "df_capfac = df_capfac.drop('cf_tech_max',axis=1)\n",
    "\n",
    "#segfrac for a chronological model\n",
    "if timeslice==0:\n",
    "    df_segfrac = df_capfac.loc[:,['season_name','time_of_day_name']].copy()\n",
    "    df_segfrac = df_segfrac.drop_duplicates().reset_index(drop=True)\n",
    "    df_segfrac.loc[:,'segfrac'] = 0\n",
    "\n",
    "    seg_frac_period_weights = time_series_mapping[['Period_Index','Rep_Period']].groupby(by='Rep_Period').sum()\n",
    "    seg_frac_period_weights = seg_frac_period_weights['Period_Index']/seg_frac_period_weights['Period_Index'].sum()\n",
    "\n",
    "    slot_periods = int(len(df_segfrac)/len(seg_frac_period_weights))\n",
    "    slots = int(len(df_segfrac)/slot_periods)\n",
    "\n",
    "    start_val = 0\n",
    "    for i in np.arange(slots):\n",
    "        mask = np.arange(start_val,start_val+slot_periods,1)\n",
    "        df_segfrac.loc[mask, 'segfrac'] = seg_frac_period_weights.iloc[i]/slot_periods\n",
    "        start_val += slot_periods\n",
    "\n",
    "#identify renewables that qualify for RPS and hydro, but not storage \n",
    "df_gen.loc[:,'renewable_nonstor'] = df_gen.loc[:,'RPS'] | df_gen.loc[:,'HYDRO'] \n",
    "df_gen.loc[df_gen['technology'].str.contains('Wind'),'renewable_nonstor'] = 1\n",
    "df_gen.loc[df_gen['technology'].str.contains('PV'),'renewable_nonstor'] = 1\n",
    "df_gen.loc[df_gen['technology'].str.contains('CSP'),'renewable_nonstor'] = 1\n",
    "df_gen.loc[df_gen['technology'].str.contains('Solar'),'renewable_nonstor'] = 1\n",
    "\n",
    "#convert heat rates of all renewables to zero\n",
    "df_gen.loc[df_gen['technology'].str.contains('Wind'),'Heat_Rate_MMBTU_per_MWh'] = 0\n",
    "df_gen.loc[df_gen['technology'].str.contains('PV'),'Heat_Rate_MMBTU_per_MWh'] = 0\n",
    "df_gen.loc[df_gen['technology'].str.contains('CSP'),'Heat_Rate_MMBTU_per_MWh'] = 0\n",
    "df_gen.loc[df_gen['technology'].str.contains('Solar'),'Heat_Rate_MMBTU_per_MWh'] = 0\n",
    "df_gen.loc[df_gen['technology'].str.contains('Geo'),'Heat_Rate_MMBTU_per_MWh'] = 0\n",
    "df_gen.loc[df_gen['technology'].str.contains('Conventional Hydroelectric'),'Heat_Rate_MMBTU_per_MWh'] = 0\n",
    "df_gen.loc[df_gen['technology'].str.contains('Small Hydroelectric'),'Heat_Rate_MMBTU_per_MWh'] = 0\n",
    "\n",
    "#estimate retirement years based on lifetime and vintage\n",
    "mask = df_gen.loc[:,'vintage']>0\n",
    "df_gen.loc[mask, 'retirement_year'] = df_gen.loc[mask, 'vintage'] + df_gen.loc[mask, 'lifetime']\n",
    "\n",
    "#update lifetime as difference between retirement year and operating year\n",
    "mask = df_gen.loc[:,'vintage'] + df_gen.loc[:,'lifetime'] <=start_year\n",
    "df_gen.loc[mask, 'lifetime'] = start_year - df_gen.loc[mask,'vintage'] + 5\n",
    "#df_gen.loc[:,'lifetime_diff'] = df_gen.loc[:,'retirement_year'] - df_gen.loc[:,'vintage']\n",
    "#df_gen.loc[:,'lifetime_diff'].fillna(0, inplace=True)\n",
    "\n",
    "#df_gen.loc[:,'lifetime'].fillna(0, inplace=True)\n",
    "#df_gen.loc[:,'lifetime'] = df_gen.loc[:, ['lifetime', 'lifetime_diff']].max(axis=1)\n",
    "\n",
    "#capacitytoactivity for a chronological model, to normalize to annual time periods, assuming min value is hours\n",
    "df_c2a = df_gen.loc[:,['tech']].drop_duplicates()\n",
    "#remove battery and ev_shifting\n",
    "df_c2a = df_c2a.loc[~df_c2a.loc[:,'tech'].str.contains('ev_'),:]\n",
    "df_c2a = df_c2a.loc[~df_c2a.loc[:,'tech'].str.contains('battery'),:]\n",
    "df_c2a = df_c2a.loc[~df_c2a.loc[:,'tech'].str.contains('pumped'),:]\n",
    "\n",
    "df_c2a.loc[:,'c2a'] = 31.536 #8760/(days*24)\n",
    "df_c2a.loc[:,'c2a_notes'] = 'from PowerGenome'\n",
    "df_c2a.insert(0,'regions',return_region(df_c2a.loc[:,'tech']))\n",
    "df_c2a.loc[:,'regions'] = df_c2a.loc[:,'regions'].str.replace('US_N','US')\n",
    "df_c2a.loc[:,'tech'] = return_tech(df_c2a.loc[:,'tech'])\n",
    "\n",
    "#investment, fixed and variable costs\n",
    "df_costs = df_gen.loc[:,['tech', 'capex_mw', 'Fixed_OM_Cost_per_MWyr', 'Var_OM_Cost_per_MWh', 'vintage', 'interconnect_annuity', 'spur_inv_mwyr']].copy()\n",
    "df_costs['interconnect_annuity'].fillna(0, inplace=True)\n",
    "df_costs['spur_inv_mwyr'].fillna(0, inplace=True)\n",
    "df_costs.loc[:,'cost_invest'] = df_costs.loc[:, 'capex_mw']/(10**3) #$/MW to #$M/GW\n",
    "df_costs.loc[:,'cost_fixed'] = (df_costs.loc[:, 'Fixed_OM_Cost_per_MWyr'])*(10**3)/(10**6) #$/MWyr to #$M/GW-yr\n",
    "df_costs.loc[:,'cost_variable'] = df_costs.loc[:, 'Var_OM_Cost_per_MWh']*(277777.78)/(10**6) #$/MWh to #$M/PJ\n",
    "df_costs.loc[:,'cost_invest_units'] = '$M/GW'\n",
    "df_costs.loc[:,'cost_invest_notes'] = 'from PowerGenome'\n",
    "df_costs.loc[:,'cost_fixed_units'] = '$M/GWyr'\n",
    "df_costs.loc[:,'cost_fixed_notes'] = 'from PowerGenome'\n",
    "df_costs.loc[:,'cost_variable_units'] = '$M/PJ'\n",
    "df_costs.loc[:,'cost_variable_notes'] = 'from PowerGenome'\n",
    "df_costs.insert(0,'regions',return_region(df_costs.loc[:,'tech']))\n",
    "df_costs.loc[:,'tech'] = return_tech(df_costs.loc[:,'tech'])\n",
    "\n",
    "df_costs['cost_fixed'] += (df_costs.loc[:, 'interconnect_annuity'])*(10**3)/(10**6) #$/MW to #$M/GW #add spur annuity cost to capital costs\n",
    "\n",
    "# df_costs['cost_invest'] += (df_costs.loc[:, 'interconnect_annuity'] + df_costs.loc[:, 'spur_inv_mwyr'])*(10**3)/(10**6) #$/MW to #$M/GW #add annuity and spur costs to capital costs\n",
    "df_cost_invest = df_costs.loc[:,['regions','tech','vintage','cost_invest','cost_invest_units','cost_invest_notes']].copy()\n",
    "df_cost_invest = df_cost_invest.loc[df_cost_invest.cost_invest>0]\n",
    "\n",
    "df_costv = df_costs.loc[:,['regions','tech','vintage','cost_variable','cost_variable_units','cost_variable_notes']].drop_duplicates()\n",
    "df_cost_variable = pd.DataFrame(np.repeat(df_costv.values, len(all_periods), axis=0), columns= df_costv.columns)\n",
    "df_cost_variable.insert(1,'periods',int((len(df_cost_variable)/len(all_periods)))*all_periods)\n",
    "df_cost_variable = df_cost_variable[df_cost_variable.vintage<=df_cost_variable.periods]\n",
    "\n",
    "df_costf = df_costs.loc[:,['regions','tech','vintage','cost_fixed','cost_fixed_units','cost_fixed_notes']]\n",
    "df_cost_fixed = pd.DataFrame(np.repeat(df_costf.values, len(all_periods), axis=0), columns= df_costf.columns)\n",
    "df_cost_fixed.insert(1,'periods',int((len(df_cost_fixed)/len(all_periods)))*all_periods)\n",
    "df_cost_fixed = df_cost_fixed[df_cost_fixed.vintage<=df_cost_fixed.periods]\n",
    "\n",
    "\n",
    "#efficiency\n",
    "df_efficiency = df_gen.loc[:,['Resource','tech', 'Heat_Rate_MMBTU_per_MWh','renewable_nonstor', 'vintage', 'flag', 'sector', 'tech_desc', 'tech_category']].copy()\n",
    "df_efficiency.rename(columns={'Resource':'input_comm'}, inplace=True)\n",
    "#convert technologies with no heat rate                               to 100% efficiency, by setting the heat rate to 3412.0/1000 MMBTU/MWh\n",
    "df_efficiency.loc[(df_efficiency.loc[:,'Heat_Rate_MMBTU_per_MWh']==0),'Heat_Rate_MMBTU_per_MWh'] = 3412.0/1000\n",
    "df_efficiency.loc[:, 'output_comm'] = df_efficiency.loc[:,'renewable_nonstor'].apply(lambda x: 'ELCP_Renewables' if x ==1 else 'ELCP')\n",
    "mask = df_efficiency.loc[:,'flag']=='ps'\n",
    "df_efficiency.loc[mask, 'input_comm'] = 'ELC'\n",
    "df_efficiency.loc[mask, 'output_comm'] = 'ELC'\n",
    "mask = (df_efficiency['tech'].str.contains('HYD')) & (df_efficiency['flag']!='ps')\n",
    "df_efficiency.loc[mask, 'output_comm' ] = 'ELCP_Renewables_nonRPS'\n",
    "df_efficiency.loc[:,'efficiency'] = 3412.0/(df_efficiency.loc[:,'Heat_Rate_MMBTU_per_MWh']*1000)\n",
    "df_efficiency.loc[:,'efficiency'].fillna(1.0, inplace=True)\n",
    "df_efficiency.drop(columns=['Heat_Rate_MMBTU_per_MWh','renewable_nonstor'], inplace=True)\n",
    "df_efficiency.loc[:,'eff_notes'] = 'from PowerGenome'\n",
    "#remove battery and ev_shifting\n",
    "df_efficiency = df_efficiency.loc[~df_efficiency.loc[:,'tech'].str.contains('ev_'),:]\n",
    "df_efficiency = df_efficiency.loc[~df_efficiency.loc[:,'tech'].str.contains('battery'),:]\n",
    "df_efficiency.insert(0,'regions',return_region(df_efficiency.loc[:,'tech']))\n",
    "df_efficiency.loc[:,'tech'] = return_tech(df_efficiency.loc[:,'tech'])\n",
    "\n",
    "#ramp up and down fractions\n",
    "df_ramp = df_gen.loc[(df_gen.loc[:,'Ramp_Up_percentage']>0) | (df_gen.loc[:,'Ramp_Dn_percentage']>0), ['regions','tech', 'Ramp_Up_percentage', 'Ramp_Dn_percentage']].copy()\n",
    "df_ramp.rename(columns={'Ramp_Up_percentage': 'ramp_up', 'Ramp_Dn_percentage': 'ramp_down'}, inplace=True)\n",
    "df_ramp.loc[:,'tech'] = return_tech(df_ramp.loc[:,'tech'])\n",
    "df_ramp = df_ramp[(df_ramp.ramp_up<1) | (df_ramp.ramp_down<1)]\n",
    "\n",
    "#lifetime\n",
    "#df_lifetime = df_gen.loc[:,['tech','lifetime']].copy()\n",
    "#df_lifetime.insert(0,'regions',return_region(df_lifetime.loc[:,'tech']))\n",
    "#df_lifetime.loc[:,'tech'] = return_tech(df_lifetime.loc[:,'tech'])\n",
    "\n",
    "#existing capacity\n",
    "df_ex_cap = df_gen.loc[:, ['tech','Existing_Cap_MW', 'vintage' ]]\n",
    "df_ex_cap = df_ex_cap.loc[df_ex_cap.loc[:,'Existing_Cap_MW']>0,:]\n",
    "df_ex_cap.loc[:,'exist_cap'] = df_ex_cap.loc[:,'Existing_Cap_MW']/1000 #GW\n",
    "df_ex_cap.drop(columns=['Existing_Cap_MW'], inplace=True)\n",
    "df_ex_cap.loc[:,'exist_cap_units'] = 'GW'\n",
    "df_ex_cap.loc[:,'exist_cap_notes'] = 'from PowerGenome'\n",
    "##FIX THIS for zero value years for EV load shifting, removing this row for now\n",
    "df_ex_cap = df_ex_cap.loc[df_ex_cap.loc[:,'vintage']!=0,:]\n",
    "df_ex_cap.insert(0,'regions',return_region(df_ex_cap.loc[:,'tech']))\n",
    "df_ex_cap.loc[:,'tech'] = return_tech(df_ex_cap.loc[:,'tech'])\n",
    "\n",
    "#discount rate\n",
    "df_wacc = df_gen.loc[(df_gen.loc[:,'wacc_real']>0), ['tech', 'vintage', 'wacc_real']].copy()\n",
    "df_wacc.rename(columns={'wacc_real': 'tech_rate'}, inplace=True)\n",
    "df_wacc.loc[:,'tech_rate_notes'] = 'from PowerGenome'\n",
    "df_wacc.insert(0,'regions',return_region(df_wacc.loc[:,'tech']))\n",
    "df_wacc.loc[:,'tech'] = return_tech(df_wacc.loc[:,'tech'])\n",
    "\n",
    "#capital recovery years\n",
    "df_cap_rec_years = df_gen.loc[(df_gen.loc[:,'cap_recovery_years']>0),['tech', 'cap_recovery_years']].copy()\n",
    "df_cap_rec_years.rename(columns={'cap_recovery_years': 'loan'}, inplace=True)\n",
    "df_cap_rec_years.loc[:,'loan_notes'] = 'from PowerGenome'\n",
    "df_cap_rec_years.insert(0,'regions',return_region(df_cap_rec_years.loc[:,'tech']))\n",
    "df_cap_rec_years.loc[:,'tech'] = return_tech(df_cap_rec_years.loc[:,'tech'])\n",
    "\n",
    "#lifetime\n",
    "df_lifetime = df_gen.loc[(df_gen.loc[:,'lifetime']>0),['tech', 'lifetime']].copy()\n",
    "df_lifetime.rename(columns={'lifetime': 'life'}, inplace=True)\n",
    "df_lifetime.loc[:,'life_notes'] = 'from NREL ReEDS 2020'\n",
    "df_lifetime.insert(0,'regions',return_region(df_lifetime.loc[:,'tech']))\n",
    "df_lifetime.loc[:,'tech'] = return_tech(df_lifetime.loc[:,'tech'])\n",
    "\n",
    "#modify fixed and variable costs based on lifetime\n",
    "df_cost_variable_merge = df_cost_variable.merge(df_lifetime, on=['regions','tech'])\n",
    "df_cost_variable_merge['retirement_year'] = df_cost_variable_merge['vintage'] + df_cost_variable_merge['life']\n",
    "df_cost_variable = df_cost_variable_merge[df_cost_variable_merge['retirement_year']>df_cost_variable_merge['periods']]\n",
    "df_cost_variable = df_cost_variable.drop(['life','life_notes','retirement_year'], axis=1)\n",
    "\n",
    "df_cost_fixed_merge = df_cost_fixed.merge(df_lifetime, on=['regions','tech'])\n",
    "df_cost_fixed_merge['retirement_year'] = df_cost_fixed_merge['vintage'] + df_cost_fixed_merge['life']\n",
    "df_cost_fixed = df_cost_fixed_merge[df_cost_fixed_merge['retirement_year']>df_cost_fixed_merge['periods']]\n",
    "df_cost_fixed = df_cost_fixed.drop(['life','life_notes','retirement_year'], axis=1)\n",
    "\n",
    "#capacity credit\n",
    "#modify capacity credit for renewables\n",
    "df_gen.loc[df_gen.tech.str.contains('WND'),'CapRes'] = 0.1\n",
    "df_gen.loc[df_gen.tech.str.contains('SOLPVCEN'),'CapRes'] = 0.1\n",
    "df_gen.loc[df_gen.tech.str.contains('SOLTHCEN_N'),'CapRes'] = 0.1\n",
    "df_gen.loc[df_gen.tech.str.contains('SOLPV_R'),'CapRes'] = 0.1\n",
    "\n",
    "df_ccredit = df_gen.loc[df_gen.loc[:,'CapRes']>0, ['tech', 'vintage', 'CapRes']].copy()\n",
    "df_ccredit  = pd.DataFrame(np.repeat(df_ccredit.values, len(all_periods), axis=0), columns= df_ccredit.columns)\n",
    "df_ccredit.insert(0,'periods',int((len(df_ccredit)/len(all_periods)))*all_periods)\n",
    "df_ccredit.rename(columns={'CapRes': 'cf_tech'}, inplace=True)\n",
    "df_ccredit.loc[:,'cf_tech_notes'] = 'from PowerGenome'\n",
    "df_ccredit.insert(0,'regions',return_region(df_ccredit.loc[:,'tech']))\n",
    "df_ccredit.loc[:,'tech'] = return_tech(df_ccredit.loc[:,'tech'])\n",
    "mask = df_ccredit.loc[:,'periods']>=df_ccredit.loc[:,'vintage']\n",
    "df_ccredit = df_ccredit.loc[mask,:]\n",
    "\n",
    "#max capacity, for renewables, primarily\n",
    "df_maxcap = df_gen.loc[(df_gen.loc[:,'Max_Cap_MW']>0),['tech','Max_Cap_MW','vintage']].copy()\n",
    "df_maxcap.rename(columns={'Max_Cap_MW':'maxcap', 'vintage':'periods'}, inplace=True)\n",
    "df_maxcap.loc[:,'maxcap'] = df_maxcap.loc[:,'maxcap']/1000 #convert MW to GW\n",
    "df_maxcap.loc[:,'maxcap_units'] = 'GW' #convert MW to GW\n",
    "df_maxcap.loc[:,'maxcap_notes'] = 'from PowerGenome'\n",
    "df_maxcap.insert(0,'regions',return_region(df_maxcap.loc[:,'tech']))\n",
    "df_maxcap.loc[:,'tech'] = return_tech(df_maxcap.loc[:,'tech'])\n",
    "\n",
    "#load, estimating demand specific distribution, using data from EFS\n",
    "if timeslice==0:\n",
    "    df_load_i = reduced_load_profile\n",
    "    ind = list(reduced_load_profile.columns).index('Time_Index')\n",
    "    df_load_i = reduced_load_profile[reduced_load_profile.columns[ind+1:]]\n",
    "    df_load_i = df_load_i.reset_index(drop=True).multiply(df_segfrac.loc[:,'segfrac'].reset_index(drop=True), axis=0)\n",
    "    if elec_only!=1:\n",
    "        df_load_i = df_load_i/df_load_i.sum()\n",
    "    iter_val = 0\n",
    "    for day in np.arange(days)+1:\n",
    "        df_load_i.loc[iter_val:iter_val+hours-1,'season_name'] = 'S' + str(int(day))\n",
    "        df_load_i.loc[iter_val:iter_val+hours-1,'time_of_day_name'] = ['H' + str(int(x)) for x in np.arange(1,hours+1)]\n",
    "        iter_val+=hours\n",
    "    df_load_i = df_load_i.melt(id_vars=['season_name','time_of_day_name'])\n",
    "    df_load_i = df_load_i.rename(columns={'variable':'regions', 'value':'dds'})\n",
    "\n",
    "    df_load = pd.DataFrame()\n",
    "    if elec_only!=1:\n",
    "        #demns = ['RSC', 'RSH','RLT','ROELC','RWH','CLT','COELC','COEELC','CSC','CSH','CWH']\n",
    "        df_load_i.loc[:, 'demand_name'] = df_load_i.loc[:, 'regions'].map(lambda x: x.split('_')[-1])\n",
    "        if single_region==1:\n",
    "            df_load_i.loc[:, 'regions'] = 'US'\n",
    "        else:\n",
    "            df_load_i.loc[:, 'regions'] = df_load_i.loc[:, 'regions'].map(lambda x: x.replace('_' + x.split('_')[-1],''))\n",
    "        df_load_i.loc[:, 'regions'] = df_load_i.loc[:, 'regions'].str.replace('US_N','US')\n",
    "        df_load = df_load_i[['regions','season_name','time_of_day_name','demand_name','dds']]\n",
    "    else:\n",
    "        demn = 'ELC_dem'\n",
    "        for region in df_load_i.regions.unique():\n",
    "            df_load_i.loc[(df_load_i.regions==region),'demand_name'] = demn\n",
    "            df_load_i.loc[(df_load_i.regions==region),'dds'] = df_load_i.loc[(df_load_i.regions==region),'dds']/df_load_i.loc[(df_load_i.regions==region),'dds'].sum()\n",
    "            df_load = pd.concat([df_load, df_load_i.loc[(df_load_i.regions==region),['regions','season_name','time_of_day_name','demand_name','dds']]])\n",
    "else:\n",
    "    df_load_i= load_curves.reset_index(drop=True)\n",
    "    df_load_i.loc[:,'hour'] = np.tile(np.arange(0,24),365)\n",
    "    df_load_i.loc[((df_load_i['hour']>=6) & (df_load_i['hour']<12)), 'time_of_day_name'] = 'am'\n",
    "    df_load_i.loc[((df_load_i['hour']>=12) & (df_load_i['hour']<15)), 'time_of_day_name'] = 'peak'\n",
    "    df_load_i.loc[((df_load_i['hour']>=15) & (df_load_i['hour']<21)), 'time_of_day_name'] = 'pm'\n",
    "    df_load_i.loc[((df_load_i['hour']>=21) | (df_load_i['hour']<6)), 'time_of_day_name'] = 'night'\n",
    "    summer = np.arange(171*24, 265*24) #June 20 to Sep 22\n",
    "    winter = np.concatenate((np.arange(0,79*24), np.arange(355*24,8760))) #dec 21st#march 20th \n",
    "    df_load_i.loc[:, 'season_name'] = 'Intermediate'\n",
    "    df_load_i.loc[summer, 'season_name'] = 'Summer'\n",
    "    df_load_i.loc[winter, 'season_name'] = 'Winter'\n",
    "    df_load = df_load_i.groupby(['time_of_day_name', 'season_name']).sum().reset_index()\n",
    "    df_load.drop(columns=['hour'], inplace=True)\n",
    "    df_load.loc[:,'regions'] = 'US'\n",
    "\n",
    "    df_load = df_load.melt(id_vars=['time_of_day_name','season_name','regions'],value_name='dds')\n",
    "    df_load.rename(columns={'variable':'demand_name'} , inplace=True)\n",
    "    df_load = df_load.merge(df_load.groupby(by=['demand_name']).sum().reset_index(), on = ['demand_name'])\n",
    "    df_load['dds'] = df_load['dds_x']/df_load['dds_y']\n",
    "    if single_region==0:\n",
    "        df_load.loc[:,'regions'] = df_load['demand_name'].str.split('_').apply(lambda x: '_'.join(x[0:-1]))\n",
    "        df_load.loc[:,'demand_name'] = df_load['demand_name'].str.split('_').apply(lambda x: x[-1])\n",
    "    else:\n",
    "        df_load.loc[:,'regions'] = 'US'\n",
    "    df_load = df_load.loc[:,['regions','season_name','time_of_day_name','demand_name','dds']]\n",
    "\n",
    "    df_segfrac = df_load_i.iloc[:,-3:].reset_index(drop=True)\n",
    "    df_segfrac.drop(columns=['hour'], inplace=True)\n",
    "    df_segfrac['segfrac'] = 1\n",
    "    df_segfrac = df_segfrac.groupby(by=['time_of_day_name','season_name']).sum().reset_index()\n",
    "    df_segfrac['segfrac'] = df_segfrac['segfrac']/df_segfrac['segfrac'].sum()\n",
    "    df_segfrac.loc[:,'segfrac_notes'] = 'from PowerGenome'  \n",
    "\n",
    "df_load.loc[:,'dds_notes'] = 'from EFS and PowerGenome'  \n",
    "\n",
    "model_regions = df_efficiency['regions'].unique()\n",
    "\n",
    "if os.path.exists(file_prefix + 'transmission.csv'):\n",
    "\n",
    "    #transmission efficiency\n",
    "    df_trans_efficiency = pd.DataFrame(columns = df_efficiency.columns)\n",
    "    df_trans_efficiency.loc[:,'regions']  = transmission.loc[:,'transmission_path_name'].str.replace('_to_','-')\n",
    "    df_trans_efficiency.loc[:,'input_comm'] = 'ELC'\n",
    "    df_trans_efficiency.loc[:,'tech'] = 'E_TRANS_R'\n",
    "    df_trans_efficiency.loc[:,'vintage'] = all_periods[0]-1\n",
    "    df_trans_efficiency.loc[:,'output_comm'] = 'ELC'\n",
    "    df_trans_efficiency.loc[:,'efficiency'] = 1 - transmission.loc[:,'Line_Loss_Percentage']\n",
    "    df_trans_efficiency.loc[:,'eff_notes'] = 'from PowerGenome'\n",
    "    df_trans_efficiency.loc[:,'flag'] = 'p'\n",
    "    df_trans_efficiency.loc[:,'sector'] = 'electric'\n",
    "    df_trans_efficiency.loc[:,'tech_desc'] = '#electricity transmission'\n",
    "    \n",
    "    df_efficiency = pd.concat([df_efficiency, df_trans_efficiency])\n",
    "    df_trans_efficiency['regions'] = df_trans_efficiency['regions'].apply(swap_regions)\n",
    "    df_efficiency = pd.concat([df_efficiency, df_trans_efficiency])\n",
    "    df_trans_efficiency_n= df_trans_efficiency.copy()\n",
    "    \n",
    "    #transmission_efficiency_new\n",
    "    df_trans_efficiency = pd.DataFrame(columns = df_efficiency.columns)\n",
    "    df_trans_efficiency.loc[:,'regions']  = transmission.loc[:,'transmission_path_name'].str.replace('_to_','-')\n",
    "    df_trans_efficiency.loc[:,'vintage'] = all_periods[0]\n",
    "    df_trans_efficiency.loc[:,'input_comm'] = 'ELC'\n",
    "    df_trans_efficiency.loc[:,'tech'] = 'E_TRANS_N'\n",
    "    df_trans_efficiency.loc[:,'output_comm'] = 'ELC'\n",
    "    df_trans_efficiency.loc[:,'efficiency'] = 1 - transmission.loc[:,'Line_Loss_Percentage']\n",
    "    df_trans_efficiency.loc[:,'eff_notes'] = 'from PowerGenome'\n",
    "    df_trans_efficiency.loc[:,'flag'] = 'p'\n",
    "    df_trans_efficiency.loc[:,'sector'] = 'electric'\n",
    "    df_trans_efficiency.loc[:,'tech_desc'] = '#new electricity transmission'\n",
    "\n",
    "    df_trans_efficiency_r = pd.DataFrame(np.repeat(df_trans_efficiency.values, len(all_periods), axis=0), columns= df_trans_efficiency.columns)\n",
    "    df_trans_efficiency_r.loc[:,'vintage'] = np.tile(all_periods, len(df_trans_efficiency))\n",
    "    \n",
    "    df_efficiency = pd.concat([df_efficiency, df_trans_efficiency_r])\n",
    "    df_trans_efficiency_r['regions'] = df_trans_efficiency_r['regions'].apply(swap_regions)\n",
    "    df_efficiency = pd.concat([df_efficiency, df_trans_efficiency_r])\n",
    "\n",
    "    #transmission investment costs \n",
    "    df_trans_c = pd.DataFrame(columns = df_cost_invest.columns)\n",
    "    df_trans_c.loc[:,'regions']  = transmission.loc[:,'transmission_path_name'].str.replace('_to_','-')\n",
    "    df_trans_c.loc[:,'vintage'] = all_periods[0]\n",
    "    df_trans_c.loc[:,'tech'] = 'E_TRANS_N'\n",
    "    #df_trans_c.loc[:,'vintage'] = all_periods[0]-1\n",
    "    df_trans_c.loc[:, 'cost_invest'] = transmission.loc[:, 'Line_Reinforcement_Cost_per_MWyr']*(10**3)/(10**6) #$/MW to #$M/GW\n",
    "    df_trans_c.loc[:, 'cost_invest_units'] = '$M/GW'\n",
    "    df_trans_c.loc[:, 'cost_invest_notes'] = 'from PowerGenome'\n",
    "    df_trans_costs = pd.DataFrame(np.repeat(df_trans_c.values, len(all_periods), axis=0), columns= df_trans_c.columns)\n",
    "    df_trans_costs.loc[:,'vintage'] = np.tile(all_periods, len(df_trans_c))\n",
    "    \n",
    "    df_cost_invest = pd.concat([df_cost_invest, df_trans_costs])\n",
    "    df_trans_costs['regions'] = df_trans_costs['regions'].apply(swap_regions)\n",
    "    df_cost_invest = pd.concat([df_cost_invest, df_trans_costs])\n",
    "    \n",
    "    #transmission investment wacc\n",
    "    df_trans_wacc = df_trans_costs[['regions', 'tech', 'vintage']].copy().drop_duplicates()\n",
    "    df_trans_wacc.loc[:,'tech_rate'] = 0.069 #from PG example system settings file\n",
    "    df_trans_wacc.loc[:, 'tech_rate_notes'] = 'from PowerGenome'\n",
    "\n",
    "    df_wacc = pd.concat([df_wacc, df_trans_wacc])\n",
    "    df_trans_wacc['regions'] = df_trans_wacc['regions'].apply(swap_regions)\n",
    "    df_wacc = pd.concat([df_wacc, df_trans_wacc])\n",
    "    \n",
    "    #new transmission lifetime\n",
    "    df_trans_lifetime = df_trans_c[['regions', 'tech']].copy().drop_duplicates()\n",
    "    df_trans_lifetime.loc[:,'life'] = 60 #from PG example system settings file\n",
    "    df_trans_lifetime.loc[:, 'life_notes'] = 'from PowerGenome'\n",
    "\n",
    "    df_lifetime = pd.concat([df_lifetime, df_trans_lifetime])\n",
    "    df_trans_lifetime['regions'] = df_trans_lifetime['regions'].apply(swap_regions)\n",
    "    df_lifetime = pd.concat([df_lifetime, df_trans_lifetime])\n",
    "\n",
    "    #trans existing capacity\n",
    "    df_trans_cap = pd.DataFrame()\n",
    "\n",
    "    df_trans_cap.loc[:,'regions']  = transmission.loc[:,'transmission_path_name'].str.replace('_to_','-')\n",
    "    df_trans_cap.loc[:,'vintage']  = all_periods[0]-1\n",
    "\n",
    "    df_trans_cap.loc[:,'exist_cap'] = abs(transmission.loc[:,['Line_Max_Flow_MW', 'Line_Min_Flow_MW']]).max(axis=1).values/1000\n",
    "\n",
    "    df_trans_cap.insert(1,'tech', 'E_TRANS_R')\n",
    "    df_trans_cap.loc[:,'exist_cap_units'] = 'GW'\n",
    "    df_trans_cap.loc[:,'exist_cap_notes'] = 'from PowerGenome'\n",
    "    \n",
    "    df_ex_cap = pd.concat([df_ex_cap, df_trans_cap])\n",
    "    df_trans_cap['regions'] = df_trans_cap['regions'].apply(swap_regions)\n",
    "    df_ex_cap = pd.concat([df_ex_cap, df_trans_cap])\n",
    "    \n",
    "    #capacity2activity\n",
    "    df_trans_gen = pd.concat([df_trans_efficiency_r, df_trans_efficiency_n])\n",
    "    df_c2a_trans = df_trans_gen[['regions','tech']].drop_duplicates()\n",
    "\n",
    "    df_c2a_trans.loc[:,'c2a'] = 31.536 #8760/(days*24)\n",
    "    df_c2a_trans.loc[:,'c2a_notes'] = 'from PowerGenome'\n",
    "    df_c2a = pd.concat([df_c2a, df_c2a_trans])\n",
    "    \n",
    "    #add co2 transport between regions to efficiency\n",
    "    if include_co2_trans==1:\n",
    "        df_co2_transport = df_trans_efficiency_r.copy()\n",
    "        df_co2_transport['efficiency']  = 0.99\n",
    "        df_co2_transport['tech']  = 'co2_TRANS_N'\n",
    "        df_co2_transport['sector'] = 'supply'\n",
    "        df_co2_transport['eff_notes'] = 'allows co2 transport between adjoining regions'\n",
    "        df_co2_transport['tec_desc'] = 'new inter-region co2 transport pipelines'\n",
    "        df_co2_transport['input_comm'] = 'co2_to_ground'\n",
    "        df_co2_transport['output_comm'] = 'co2_to_ground'\n",
    "        df_co2_transport_tx = df_co2_transport[df_co2_transport['regions'].str.contains('TX')].copy()\n",
    "        for regs in ['SW', 'SE']:\n",
    "            df_co2_transport_mod = df_co2_transport_tx.copy()\n",
    "            df_co2_transport_mod['regions'] = df_co2_transport_mod['regions'].str.replace('CEN',regs)\n",
    "            df_co2_transport = pd.concat([df_co2_transport, df_co2_transport_mod ])\n",
    "        df_efficiency = pd.concat([df_efficiency, df_co2_transport])\n",
    "        df_co2_transport['regions'] = df_co2_transport['regions'].apply(swap_regions)\n",
    "        df_efficiency = pd.concat([df_efficiency, df_co2_transport])\n",
    "\n",
    "\n",
    "#size of individual units in new technologies cluster\n",
    "df_technology_new_cluster = df_gen.loc[df_gen['Existing_Cap_MW'].isna(),['tech', 'Cap_size']]\n",
    "df_technology_new_cluster.insert(0,'regions' ,return_region(df_technology_new_cluster.loc[:,'tech']))\n",
    "df_technology_new_cluster['tech'] =  return_tech(df_technology_new_cluster.loc[:,'tech'])\n",
    "df_technology_new_cluster.rename(columns={'Cap_size':'cap_size'}, inplace=True)\n",
    "df_technology_new_cluster['cap_size'] /= 1000 #convert MW to GW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#add input_comm from commodities list\n",
    "tech_wo_clusters = df_efficiency['tech'].map(lambda x: '_'.join(x.split('_')[0:-1]) if x.split('_')[-1].isnumeric() else x)\n",
    "df_efficiency.loc[:,'input_comm'] = tech_wo_clusters.map({k: v[0] for k,v in input_table.items()})\n",
    "\n",
    "#duplicate bio to have multiple inputs\n",
    "bio = df_efficiency[df_efficiency.loc[:,'tech']=='E_BIO_N']\n",
    "bio.loc[:, 'input_comm'] = input_table['E_BIO_N'][1]\n",
    "df_efficiency = pd.concat([df_efficiency, bio])\n",
    "bio.loc[:, 'input_comm'] = input_table['E_BIO_N'][2]\n",
    "df_efficiency = pd.concat([df_efficiency, bio])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#add nox and SO2 emissions to other electric power plants\n",
    "#technologies affected:\n",
    "conn = sqlite3.connect(emptydB)\n",
    "c = conn.cursor()\n",
    "techs_so2 = ['E_NGASTM_R', 'E_NGACT_R']\n",
    "df_emiss = pd.read_sql(\"SELECT * FROM EmissionActivity WHERE emis_comm='so2_ELC' AND tech in ('\" + \"','\".join(list(techs_so2)) +\\\n",
    "                       \"' )\", conn)\n",
    "#query = \"DELETE FROM EmissionActivity WHERE emis_comm='co2' AND tech in \\\n",
    "#('\" + \"','\".join(list(techs_so2)) + \"' )\"\n",
    "emis_group = df_emiss.groupby('tech').min()\n",
    "\n",
    "df_new_emiss = pd.DataFrame()\n",
    "for tech in techs_so2:\n",
    "    emis_techs = df_efficiency.loc[df_efficiency['tech'].str.contains(tech), :]\n",
    "    emis_techs.loc[:,'emis_act'] = emis_group.loc[emis_group.index.str.contains(tech), 'emis_act'].values[0]\n",
    "    emis_techs['emis_comm'] = 'so2_ELC'\n",
    "    emis_techs['emis_act_units'] = 'kt/PJout'\n",
    "    emis_techs['emis_act_notes'] = 'from original national db'\n",
    "    df_new_emiss = pd.concat([df_new_emiss,emis_techs ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "techs_nox = ['E_NGASTM_R', 'E_NGACT_R', 'E_NGACC_R', 'E_NGACC_CCS_N']\n",
    "\n",
    "df_emiss = pd.read_sql(\"SELECT * FROM EmissionActivity WHERE emis_comm='nox_ELC' AND tech in ('\" + \"','\".join(list(techs_nox)) +\\\n",
    "                       \"' )\", conn)\n",
    "emis_group = df_emiss.groupby('tech').min()\n",
    "for tech in techs_nox:\n",
    "    emis_techs = df_efficiency.loc[df_efficiency['tech'].str.contains(tech), :]\n",
    "    emis_techs.loc[:,'emis_act'] = emis_group.loc[emis_group.index.str.contains(tech), 'emis_act'].values[0]\n",
    "    emis_techs['emis_comm'] = 'nox_ELC'\n",
    "    emis_techs['emis_act_units'] = 'kt/PJout'\n",
    "    emis_techs['emis_act_notes'] = 'from original national db'\n",
    "    if tech=='E_NGACC_CCS_N':\n",
    "        emis_techs['tech'] = 'E_NGACC_CCS_ZERO_N'\n",
    "    df_new_emiss = pd.concat([df_new_emiss,emis_techs ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#add CO2 output commodity to the efficiency tables\n",
    "# co2 emissions factors from here: https://www.eia.gov/tools/faqs/faq.php?id=73&t=11\n",
    "emiss_dict = {}\n",
    "emiss_dict['COALSTM_N'] = 88.42 #using the min of what's in the db at this time 205.7/2.2/(9.478e+5)*1000*1000 # lbs/MMBTU to kt/PJ\n",
    "emiss_dict['E_NGA'] = 50 # using the min of what's in the db 117.0/2.2/(9.478e+5)*1000*1000 # lbs/MMBTU to kt/PJ\n",
    "emiss_dict['COALIGCC_N'] = 88.42 #using the min of what's in the db at this time 205.7/2.2/(9.478e+5)*1000*1000 # lbs/MMBTU to kt/PJ\n",
    "\n",
    "mask = df_efficiency['tech'].str.contains('CCS')\n",
    "ccs_techs = df_efficiency.loc[mask, :].copy()\n",
    "ccs_techs_emissions = df_efficiency.loc[mask, :].copy()\n",
    "ccs_techs.loc[:,'fuel_emis'] = ccs_techs['input_comm'].map(emiss_dict)\n",
    "ccs_techs.loc[:,'input_comm'] = 'ethos'\n",
    "ccs_techs.loc[:,'output_comm'] = 'co2_CCS'\n",
    "ccs_techs.loc[:,'tech'] = ccs_techs.loc[:,'tech'] + '_emissions'\n",
    "\n",
    "ccs_techs.loc[:,'co2_removal_eff'] = 0.9\n",
    "ccs_techs.loc[(ccs_techs.loc[:,'tech'].str.contains('ZERO')),'co2_removal_eff'] = 1\n",
    "ccs_techs.loc[:, 'eff_notes'] = 'associated dummy process to account for co2 as a physical commodity'\n",
    "\n",
    "#ccs_techs.loc[:,'to_split'] = 1 - ccs_techs.loc[:,'efficiency']/( ccs_techs.loc[:,'efficiency']+ (ccs_techs.loc[:,'co2_removal_eff']*ccs_techs.loc[:,'fuel_emis']))\n",
    "\n",
    "#modify the outputs of all ccs techs to be PJ + co2 emissions\n",
    "#df_efficiency.loc[mask,'efficiency'] = ccs_techs.loc[:,'efficiency']+  ccs_techs.loc[:,'co2_removal_eff']*ccs_techs.loc[:,'fuel_emis']\n",
    "ccs_techs.loc[:,'efficiency'] = 1 #ccs_techs.loc[:,'co2_removal_eff']*ccs_techs.loc[:,'fuel_emis']\n",
    "\n",
    "df_efficiency = pd.concat([df_efficiency, ccs_techs[df_efficiency.columns]])\n",
    "\n",
    "# add CO2 output to the emissionsactivity tables\n",
    "\n",
    "ccs_techs_emissions.loc[:,'fuel_emis'] = ccs_techs_emissions['input_comm'].map(emiss_dict)\n",
    "ccs_techs_emissions.loc[:,'co2_removal_eff'] = 0.9\n",
    "ccs_techs_emissions.loc[(ccs_techs_emissions.loc[:,'tech'].str.contains('ZERO')),'co2_removal_eff'] = 1\n",
    "ccs_techs_emissions.insert(1,'emis_comm','co2')\n",
    "ccs_techs_emissions['emis_act'] = -ccs_techs_emissions.loc[:,'co2_removal_eff']*ccs_techs_emissions.loc[:,'fuel_emis']/ccs_techs_emissions.loc[:,'efficiency']\n",
    "ccs_techs_emissions['emis_act_units'] = 'kt/PJout'\n",
    "ccs_techs_emissions['emis_act_notes'] = 'negative emissions from capturing the carbon'\n",
    "#ccs_techs['to_split_notes'] = 'TechOutputSplit for ccs techs  is calculated as (A/(A+B)) or (B/(A+B)) where A and B are the original efficiencies for the technology  co2_ccs and ELCP (or ELCP_Renewables)'\n",
    "ccs_techs_emissions['periods'] = ccs_techs_emissions['vintage']\n",
    "\n",
    "conn = sqlite3.connect(outputdB)\n",
    "c = conn.cursor()\n",
    "df_emiss = pd.read_sql(\"SELECT * FROM EmissionActivity WHERE emis_comm='co2' AND tech in ('\" + \"','\".join(list(ccs_techs_emissions.tech.unique())) +\\\n",
    "                       \"' )\", conn)\n",
    "query = \"DELETE FROM EmissionActivity WHERE emis_comm='co2' AND tech in \\\n",
    "('\" + \"','\".join(list(ccs_techs_emissions.tech.unique())) + \"' )\"\n",
    "c.execute(query)\n",
    "\n",
    "\n",
    "#update lifetimes\n",
    "mask = df_lifetime['tech'].str.contains('CCS')\n",
    "ccs_tech_lifetimes = df_lifetime.loc[mask, :].copy()\n",
    "ccs_tech_lifetimes.loc[:,'tech'] = ccs_tech_lifetimes.loc[:,'tech'] + '_emissions'\n",
    "df_lifetime = pd.concat([df_lifetime, ccs_tech_lifetimes])\n",
    "\n",
    "#update linked techs\n",
    "df_linkedtechs = ccs_techs_emissions[['regions','tech']].drop_duplicates()\n",
    "df_linkedtechs.columns = 'primary_' + df_linkedtechs.columns.str.replace('regions','region')\n",
    "df_linkedtechs['linked_tech'] = df_linkedtechs['primary_tech'] + '_emissions'\n",
    "df_linkedtechs.insert(2,'emis_comm','co2')\n",
    "df_linkedtechs['tech_linked_notes'] = 'from PowerGenome'\n",
    "query = \"DELETE FROM LinkedTechs WHERE primary_tech IN ('\" + \"','\".join(df_linkedtechs['primary_tech'].values) + \"')\" \n",
    "c.execute(query)\n",
    "conn.commit()\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#change existing wind and solar efficiencies to 1\n",
    "df_efficiency.loc[(df_efficiency.loc[:,'tech'].str.contains('wind')) & (df_efficiency.loc[:,'vintage']<start_year), 'efficiency'] = 1\n",
    "df_efficiency.loc[(df_efficiency.loc[:,'tech'].str.contains('solar')) & (df_efficiency.loc[:,'vintage']<start_year), 'efficiency'] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#change output_comm for residential PV to ELCDIST\n",
    "df_efficiency.loc[df_efficiency.loc[:,'tech']=='E_SOLPVENDUSE_N', 'output_comm'] = 'ELCDIST_R'\n",
    "respv = df_efficiency.loc[df_efficiency.loc[:,'tech']=='E_SOLPVENDUSE_N',:]\n",
    "\n",
    "for reg in respv['regions'].unique():\n",
    "    \n",
    "    #pick the 5 largest clusters\n",
    "    mask1 = df_maxcap['regions']==reg\n",
    "    mask2 = df_maxcap['tech'].str.contains('E_SOLPVCEN')\n",
    "    df_solar_maxcap = df_maxcap.loc[mask1 & mask2 & (df_maxcap['periods']==start_year)]\n",
    "    df_solar_maxcap = df_solar_maxcap.nlargest(5,'maxcap')\n",
    "\n",
    "    mask1 = df_capfac['regions']==reg\n",
    "    mask2 = df_capfac['tech'].str.contains('E_SOLPVCEN')\n",
    "    df_capfac_sol_reg = df_capfac[mask1 & mask2]\n",
    "    df_capfac_sol_reg = df_capfac_sol_reg.merge(df_segfrac.drop_duplicates())\n",
    "    df_capfac_sol_reg['weighted_cf'] = df_capfac_sol_reg['cf_tech']*df_capfac_sol_reg['segfrac']\n",
    "    \n",
    "    #pick the cluster with the highest CF\n",
    "    df_solar = df_capfac_sol_reg.groupby(by='tech')['weighted_cf'].sum().reset_index()\n",
    "    df_solar = df_solar[df_solar['tech'].isin(df_solar_maxcap['tech'])]\n",
    "    sel_solar_tech = df_solar.loc[df_solar['weighted_cf'].idxmax()]['tech']\n",
    "\n",
    "    respv = df_capfac.loc[(df_capfac['regions']==reg) & (df_capfac['tech']==sel_solar_tech)]\n",
    "    respv.loc[:,'tech']= 'E_SOLPVENDUSE_N'\n",
    "    \n",
    "    df_capfac = pd.concat([df_capfac, respv])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#add min residential PV generation to MinGenGroupWeight table\n",
    "df_target = pd.DataFrame()\n",
    "df_gen_respv = pd.read_csv('electric/Residential_PV_gen_GWh.csv')\n",
    "if single_region==1:\n",
    "    df_gen_respv = df_gen_respv.groupby(['year']).sum().reset_index()\n",
    "    df_gen_respv['reg_names'] = 'US'\n",
    "for reg in model_regions:\n",
    "    df_reg = df_gen_respv[df_gen_respv.reg_names==reg]\n",
    "    df_reg = df_reg[['reg_names', 'year', 'value']]\n",
    "    df_reg['value'] *= 0.0036 #convert GWh to PJ\n",
    "    df_target = pd.concat([df_target, df_reg ])\n",
    "    \n",
    "df_target['value'] = np.round(df_target['value'],2)\n",
    "df_target = df_target.rename(columns={'year':'periods','reg_names':'regions','value':'minact'})\n",
    "df_target.insert(2,'tech','E_SOLPVENDUSE_N')\n",
    "df_target.insert(4,'minact_units','PJ')\n",
    "\n",
    "df_target.loc[:,'minact_notes'] = 'From NREL dGen model mid PV cost scenario'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_blend_target = df_target.copy()\n",
    "df_blend_target['maxact'] = df_blend_target['minact']*1.2\n",
    "df_blend_target = df_blend_target[['regions','periods','tech','maxact']]\n",
    "df_blend_target['maxact_units'] = 'PJ'\n",
    "df_blend_target['maxact_notes'] = 'From NREL dGen model mid PV cost scenario, 120%'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#add capacity factor data for solar thermal technologies\n",
    "csp = df_capfac.loc[df_capfac.loc[:,'tech']=='E_SOLPVCEN_N_1',:]\n",
    "csp.loc[:,'tech'] = csp.loc[:,'tech'].str.replace('E_SOLPVCEN_N_1', 'E_SOLTHCEN_N')\n",
    "csp = csp[(csp['regions']=='CA') | (csp['regions']=='SW')] #solar thermal only in CA and SW\n",
    "csp.loc[:,'cf_tech'] = 0.64\n",
    "df_capfac = pd.concat([df_capfac, csp])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#add RPS constraints\n",
    "df_rps = pd.read_csv('electric/FEDERAL_Updated Regional Renewable Portfolio Standards 2000-2050.csv')\n",
    "df_rps.set_index('Region', inplace=True)\n",
    "df_rps = df_rps.loc[:,[str(x) for x in all_periods]]\n",
    "df_rps = df_rps[df_rps.index!='Federal']\n",
    "\n",
    "region_names = pd.read_csv('region_names.csv', header=None, index_col=0)\n",
    "region_names = region_names.to_dict()[1]\n",
    "\n",
    "df_rps.index = [region_names[int(x)] for x in df_rps.index]\n",
    "df_rps = df_rps.apply(lambda x: x.str.replace('%', '').astype(float)/100, axis=1)\n",
    "df_rps = df_rps.reset_index().melt(id_vars='index')\n",
    "df_rps = df_rps.rename(columns={'index':'regions', 'variable':'periods', 'value':'ti_split'})\n",
    "df_rps.insert(2, 'input_comm','ELCP_Renewables')\n",
    "df_rps.insert(3, 'tech','E_ELCTDLOSS')\n",
    "df_rps.loc[:,'ti_split_notes'] = 'Average approximate RPS representation'\n",
    "\n",
    "df_rps = df_rps[df_rps['regions'].isin(model_regions)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#storage duration to existing pumped hydro\n",
    "df_stordur = df_efficiency[(df_efficiency['flag']=='ps') & (df_efficiency['tech'].str.contains('HYD'))]\n",
    "df_stordur = df_stordur[['regions','tech']].drop_duplicates()\n",
    "if len(df_stordur)>0:\n",
    "    df_stordur.loc[:, 'duration'] = 10 #assuming 10 hours\n",
    "    df_stordur.loc[:, 'duration_notes'] = 'assuming 10 hours for all pumped hydro storage'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#add 90% availability factor to nuclear power plants\n",
    "df_capfac_nuclear = df_capfac[(df_capfac['tech']==df_capfac.iloc[0]['tech']) & (df_capfac['regions']==df_capfac.iloc[0]['regions'])]\n",
    "df_capfac_nuclear.loc[:,'cf_tech'] = 0.9\n",
    "df_capfac_nuclear.loc[:,'cf_tech_notes'] = 'EPA IPM assumptions'\n",
    "df_capfac_nuclear_all = pd.DataFrame()\n",
    "for ind, row in df_efficiency.loc[df_efficiency['tech'].str.contains('URN'),['regions','tech']].iterrows():\n",
    "    df_capfac_nuclear.loc[:,'tech'] = row['tech']\n",
    "    df_capfac_nuclear.loc[:,'regions'] = row['regions']\n",
    "    df_capfac_nuclear_all = pd.concat([df_capfac_nuclear_all, df_capfac_nuclear])\n",
    "df_capfac = pd.concat([df_capfac, df_capfac_nuclear_all])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#add 10% ITC to centralized solar pv \n",
    "mask = df_cost_invest['tech'].str.contains('E_SOLPVCEN')\n",
    "df_cost_invest.loc[mask, 'cost_invest'] *=0.9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#WRITE TO DATABASE\n",
    "#also replace all the e_coalstm_n techs by e_coalstm_r techs\n",
    "conn = sqlite3.connect(outputdB)\n",
    "\n",
    "#CapacityToActivity\n",
    "df_c2a = df_c2a.drop_duplicates()\n",
    "df_table = df_c2a[~df_c2a['tech'].isin(old_df_c2a['tech'])]\n",
    "df_table['tech'] = df_table['tech'].str.replace('E_COALSTM_N_', 'E_COALSTM_R_')\n",
    "sqlite_table = 'CapacityToActivity'\n",
    "df_table.to_sql(sqlite_table, conn, if_exists='append', index=False)\n",
    "\n",
    "#SegFrac\n",
    "# if timeslice==0:\n",
    "df_table = df_segfrac.drop_duplicates()\n",
    "sqlite_table = 'SegFrac'\n",
    "df_table.to_sql(sqlite_table, conn, if_exists='append', index=False)\n",
    "\n",
    "#CostInvest\n",
    "df_table = df_cost_invest.copy()\n",
    "df_table = df_table.loc[df_table.loc[:,'cost_invest']>0,:]\n",
    "df_table['tech'] = df_table['tech'].str.replace('E_COALSTM_N_', 'E_COALSTM_R_')\n",
    "sqlite_table = 'CostInvest'\n",
    "df_table.to_sql(sqlite_table, conn, if_exists='append', index=False)\n",
    "\n",
    "#CostVariable\n",
    "df_table = df_cost_variable.drop_duplicates()\n",
    "df_table['tech'] = df_table['tech'].str.replace('E_COALSTM_N_', 'E_COALSTM_R_')\n",
    "sqlite_table = 'CostVariable'\n",
    "df_table.to_sql(sqlite_table, conn, if_exists='append', index=False)\n",
    "\n",
    "#CostFixed\n",
    "df_table = df_cost_fixed.drop_duplicates()\n",
    "df_table['tech'] = df_table['tech'].str.replace('E_COALSTM_N_', 'E_COALSTM_R_')\n",
    "sqlite_table = 'CostFixed'\n",
    "df_table.to_sql(sqlite_table, conn, if_exists='append', index=False)\n",
    "\n",
    "#Efficiency\n",
    "df_table = df_efficiency[['regions', 'input_comm','tech','vintage','output_comm', 'efficiency', 'eff_notes']].drop_duplicates()\n",
    "df_table['tech'] = df_table['tech'].str.replace('E_COALSTM_N_', 'E_COALSTM_R_')\n",
    "sqlite_table = 'Efficiency'\n",
    "df_table.to_sql(sqlite_table, conn, if_exists='append', index=False)\n",
    "\n",
    "#tech_curtailment\n",
    "df_table = pd.DataFrame(df_efficiency.loc[['WND' in x or 'PV' in x or 'SOL' in x for x in df_efficiency.loc[:,'tech']],'tech'].drop_duplicates())\n",
    "df_table = df_table[~df_table['tech'].str.contains('BLND')]\n",
    "df_table.insert(1,'notes','')\n",
    "sqlite_table = 'tech_curtailment'\n",
    "df_table.to_sql(sqlite_table, conn, if_exists='append', index=False)\n",
    "\n",
    "#tech_production in technologies table\n",
    "df_table = df_efficiency[['tech', 'flag', 'sector', 'tech_desc', 'tech_category']].drop_duplicates()\n",
    "df_table = df_table[~df_table['tech'].isin(tech_table['tech_technologies'])]\n",
    "df_table['tech'] = df_table['tech'].str.replace('E_COALSTM_N_', 'E_COALSTM_R_')\n",
    "sqlite_table = 'technologies'\n",
    "df_table.to_sql(sqlite_table, conn, if_exists='append', index=False)\n",
    "\n",
    "#tech_ramping\n",
    "df_table = df_ramp.loc[:,['tech']].drop_duplicates()\n",
    "df_table['tech'] = df_table['tech'].str.replace('E_COALSTM_N_', 'E_COALSTM_R_')\n",
    "sqlite_table = 'tech_ramping'\n",
    "if timeslice!=1:\n",
    "    df_table.to_sql(sqlite_table, conn, if_exists='append', index=False)\n",
    "\n",
    "#ramp up\n",
    "df_table = df_ramp.loc[:,['regions','tech', 'ramp_up']].drop_duplicates()\n",
    "df_table['tech'] = df_table['tech'].str.replace('E_COALSTM_N_', 'E_COALSTM_R_')\n",
    "sqlite_table = 'RampUp'\n",
    "if timeslice!=1:\n",
    "    df_table.to_sql(sqlite_table, conn, if_exists='append', index=False)\n",
    "\n",
    "#ramp down\n",
    "df_table = df_ramp.loc[:,['regions','tech', 'ramp_down']].drop_duplicates()\n",
    "df_table['tech'] = df_table['tech'].str.replace('E_COALSTM_N_', 'E_COALSTM_R_')\n",
    "sqlite_table = 'RampDown'\n",
    "if timeslice!=1:\n",
    "    df_table.to_sql(sqlite_table, conn, if_exists='append', index=False)\n",
    "\n",
    "#ExistingCapacity\n",
    "df_table = df_ex_cap.drop_duplicates()\n",
    "df_table['tech'] = df_table['tech'].str.replace('E_COALSTM_N_', 'E_COALSTM_R_')\n",
    "sqlite_table = 'ExistingCapacity'\n",
    "df_table.to_sql(sqlite_table, conn, if_exists='append', index=False)\n",
    "\n",
    "#DiscountRate\n",
    "df_table = df_wacc.loc[:,['regions','tech', 'vintage', 'tech_rate', 'tech_rate_notes']].drop_duplicates()\n",
    "df_table['tech'] = df_table['tech'].str.replace('E_COALSTM_N_', 'E_COALSTM_R_')\n",
    "sqlite_table = 'DiscountRate'\n",
    "df_table.to_sql(sqlite_table, conn, if_exists='append', index=False)\n",
    "\n",
    "#LifetimeLoanTech\n",
    "df_table = df_cap_rec_years.drop_duplicates()\n",
    "df_table['tech'] = df_table['tech'].str.replace('E_COALSTM_N_', 'E_COALSTM_R_')\n",
    "sqlite_table = 'LifetimeLoanTech'\n",
    "df_table.to_sql(sqlite_table, conn, if_exists='append', index=False)\n",
    "\n",
    "#LifetimeTech\n",
    "c = conn.cursor()\n",
    "df_lifetime_current = pd.read_sql_query(\"SELECT * FROM LifetimeTech\", conn)\n",
    "nonoverlap_tech = df_lifetime_current[~df_lifetime_current['tech'].isin(df_lifetime['tech'])]\n",
    "query = \"DELETE FROM LifetimeTech\"\n",
    "c.execute(query)\n",
    "#df_lifetime.loc[:,'life']=120 #all to 120 years for now\n",
    "\n",
    "df_table = pd.concat([nonoverlap_tech, df_lifetime.drop_duplicates()])\n",
    "df_table['tech'] = df_table['tech'].str.replace('E_COALSTM_N_', 'E_COALSTM_R_')\n",
    "sqlite_table = 'LifetimeTech'\n",
    "df_table.to_sql(sqlite_table, conn, if_exists='append', index=False)\n",
    "\n",
    "#CapacityCredit\n",
    "df_table = df_ccredit.loc[:,['regions','periods','tech', 'vintage','cf_tech','cf_tech_notes']].drop_duplicates()\n",
    "df_table['tech'] = df_table['tech'].str.replace('E_COALSTM_N_', 'E_COALSTM_R_')\n",
    "sqlite_table = 'CapacityCredit'\n",
    "df_table.to_sql(sqlite_table, conn, if_exists='append', index=False)\n",
    "\n",
    "#tech_reserve\n",
    "df_table = df_ccredit.loc[:,['tech','cf_tech_notes']].drop_duplicates(subset='tech')\n",
    "df_table = df_table[~df_table['tech'].isin(tech_table['tech_technologies'])]\n",
    "df_table.rename(columns={'cf_tech_notes':'notes'}, inplace=True)\n",
    "df_table['tech'] = df_table['tech'].str.replace('E_COALSTM_N_', 'E_COALSTM_R_')\n",
    "sqlite_table = 'tech_reserve'\n",
    "df_table.to_sql(sqlite_table, conn, if_exists='append', index=False)\n",
    "\n",
    "#CapacityFactorTech\n",
    "df_table = df_capfac.drop_duplicates()\n",
    "df_table['tech'] = df_table['tech'].str.replace('E_COALSTM_N_', 'E_COALSTM_R_')\n",
    "df_table.loc[df_table['cf_tech']<0.01,'cf_tech']=0\n",
    "df_table.loc[df_table['cf_tech']>1,'cf_tech']=1\n",
    "\n",
    "sqlite_table = 'CapacityFactorTech'\n",
    "df_table.to_sql(sqlite_table, conn, if_exists='append', index=False)\n",
    "\n",
    "#write_sql(df_table, sqlite_table, outputdB)\n",
    "\n",
    "#MaxCapacity\n",
    "df_table = df_maxcap.loc[:,['regions','periods','tech', 'maxcap','maxcap_units','maxcap_notes']].drop_duplicates()\n",
    "df_table['tech'] = df_table['tech'].str.replace('E_COALSTM_N_', 'E_COALSTM_R_')\n",
    "sqlite_table = 'MaxCapacity'\n",
    "df_table.to_sql(sqlite_table, conn, if_exists='append', index=False)\n",
    "\n",
    "#DSD\n",
    "if timeslice==0:\n",
    "    df_table = df_load.loc[:,['regions','season_name','time_of_day_name','demand_name','dds','dds_notes']]\n",
    "    sqlite_table = 'DemandSpecificDistribution'\n",
    "    df_table.to_sql(sqlite_table, conn, if_exists='append', index=False)\n",
    "\n",
    "    #time_season\n",
    "    df_table = pd.DataFrame(df_capfac.loc[:,'season_name'].drop_duplicates())\n",
    "    df_table.rename(columns={'season_name':'t_season'},inplace=True)\n",
    "    sqlite_table = 'time_season'\n",
    "    df_table.to_sql(sqlite_table, conn, if_exists='append', index=False)\n",
    "\n",
    "    #time_of_day\n",
    "    df_table = pd.DataFrame(df_capfac.loc[:,'time_of_day_name'].drop_duplicates())\n",
    "    df_table.rename(columns={'time_of_day_name':'t_day'},inplace=True)\n",
    "    sqlite_table = 'time_of_day'\n",
    "    df_table.to_sql(sqlite_table, conn, if_exists='append', index=False)\n",
    "else:\n",
    "    df_table = df_load\n",
    "    sqlite_table = 'DemandSpecificDistribution'\n",
    "    df_table.to_sql(sqlite_table, conn, if_exists='append', index=False)\n",
    "\n",
    "#time_periods\n",
    "df_table = pd.DataFrame(columns=['t_periods','flag'])\n",
    "#df_table.loc[:,'t_periods'] = np.sort(df_costs.loc[:,'vintage'].unique().astype(int))\n",
    "df_table.loc[:,'t_periods'] = np.hstack((np.arange(df_costs.loc[:,'vintage'].astype(int).min()-1,2020), \n",
    "                                         np.arange(2020,2056,5)))\n",
    "df_table.loc[:,'flag'] = ['e' if x <2020  else 'f' for x in df_table.loc[:,'t_periods']]\n",
    "sqlite_table = 'time_periods'\n",
    "df_table.to_sql(sqlite_table, conn, if_exists='append', index=False)\n",
    "\n",
    "# #MinGenGroupWeight\n",
    "# df_table = df_weights\n",
    "# df_table['tech'] = df_table['tech'].str.replace('E_COALSTM_N_', 'E_COALSTM_R_')\n",
    "# sqlite_table = 'MinGenGroupWeight'\n",
    "# df_table.to_sql(sqlite_table, conn, if_exists='append', index=False)\n",
    "\n",
    "#maxactivity\n",
    "df_table = df_blend_target\n",
    "sqlite_table = 'MaxActivity'\n",
    "df_table.to_sql(sqlite_table, conn, if_exists='append', index=False)\n",
    "\n",
    "# #MinGenGroupTarget\n",
    "# df_table = df_target\n",
    "# sqlite_table = 'MinGenGroupTarget'\n",
    "# df_table.to_sql(sqlite_table, conn, if_exists='append', index=False)\n",
    "\n",
    "#MinActivity\n",
    "df_table = df_target\n",
    "sqlite_table = 'MinActivity'\n",
    "df_table.to_sql(sqlite_table, conn, if_exists='append', index=False)\n",
    "\n",
    "#TechInputSplitAverage\n",
    "if single_region==0:\n",
    "    df_table = df_rps\n",
    "    sqlite_table = 'TechInputSplitAverage'\n",
    "    df_table.to_sql(sqlite_table, conn, if_exists='append', index=False)\n",
    "    \n",
    "#StorageDuration\n",
    "df_table = df_stordur\n",
    "sqlite_table = 'StorageDuration'\n",
    "df_table.to_sql(sqlite_table, conn, if_exists='append', index=False)\n",
    "\n",
    "# #groups\n",
    "# df_table = pd.DataFrame(columns=['group_name', 'notes'])\n",
    "# df_table.loc[:,'group_name'] = df_weights['group_name'].unique()\n",
    "# sqlite_table = 'groups'\n",
    "# df_table.to_sql(sqlite_table, conn, if_exists='append', index=False)\n",
    "\n",
    "# #tech_groups\n",
    "# df_table = pd.DataFrame(columns=['tech', 'notes'])\n",
    "# df_table.loc[:,'tech'] = df_weights['tech'].unique()\n",
    "# df_table['tech'] = df_table['tech'].str.replace('E_COALSTM_N_', 'E_COALSTM_R_')\n",
    "# sqlite_table = 'tech_groups'\n",
    "# df_table.to_sql(sqlite_table, conn, if_exists='append', index=False)\n",
    "\n",
    "if single_region==0 & ('E_TRANS_R' in df_efficiency.tech.unique()):\n",
    "    df_table = pd.DataFrame(columns = ['tech', 'notes'])\n",
    "    df_table.loc[0] = ['E_TRANS_R','#existing electric transmission']\n",
    "    df_table.loc[1] = ['E_TRANS_N','#new electric transmission']\n",
    "    if include_co2_trans==1:\n",
    "        df_table.loc[2] = ['co2_TRANS_N','new inter-region co2 transport pipelines']\n",
    "    sqlite_table = 'tech_exchange'\n",
    "    df_table.to_sql(sqlite_table, conn, if_exists='append', index=False)\n",
    "\n",
    "#write CO2, nox_elc and so2_elc emissions for CCS technologies to EmissionActivity table\n",
    "df_table = pd.concat([df_new_emiss, ccs_techs_emissions])\n",
    "df_table = df_table[df_emiss.columns]\n",
    "df_table['tech'] = df_table['tech'].str.replace('E_COALSTM_N_', 'E_COALSTM_R_')\n",
    "sqlite_table = 'EmissionActivity'\n",
    "df_table.to_sql(sqlite_table, conn, if_exists='append', index=False)\n",
    "\n",
    "# #write techoutputsplit\n",
    "# df_table = df_tosplit_ccs\n",
    "# df_table['tech'] = df_table['tech'].str.replace('E_COALSTM_N_', 'E_COALSTM_R_')\n",
    "# sqlite_table = 'TechOutputSplit'\n",
    "# write_sql(df_table, sqlite_table, outputdB)\n",
    "\n",
    "#new table tech_new_cluster\n",
    "\n",
    "c.execute(\"CREATE TABLE IF NOT EXISTS tech_new_cluster ( regions TEXT, tech TEXT, cap_size REAL, PRIMARY KEY (regions, tech))\")\n",
    "\n",
    "sqlite_table = 'tech_new_cluster'\n",
    "df_table = df_technology_new_cluster.drop_duplicates()\n",
    "df_table['tech'] = df_table['tech'].str.replace('E_COALSTM_N_', 'E_COALSTM_R_')\n",
    "df_table.to_sql(sqlite_table, conn, if_exists='append', index=False)\n",
    "\n",
    "#update linkedtechs table to delete techs not in df_technologies\n",
    "df_table = df_linkedtechs\n",
    "sqlite_table = 'LinkedTechs'\n",
    "df_table.to_sql(sqlite_table, conn, if_exists='append', index=False)\n",
    "\n",
    "#remove any pb (baseload) flags\n",
    "if timeslice!=1:\n",
    "    query = \"UPDATE technologies SET flag='p' WHERE flag='pb'\"\n",
    "    c.execute(query)\n",
    "\n",
    "conn.commit()\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#map regions to descriptions\n",
    "if single_region!=1:\n",
    "    map_region_desc = dict()\n",
    "    map_region_desc['CA'] = 'California'\n",
    "    map_region_desc['NW'] = 'Northwestern US'\n",
    "    map_region_desc['SW'] =  'Southwestern US'\n",
    "    map_region_desc['TX'] =  'Texas'\n",
    "    map_region_desc['N_CEN'] =  'North Central US'\n",
    "    map_region_desc['CEN'] =  'Central US'\n",
    "    map_region_desc['SE'] =  'Southeastern US'\n",
    "    map_region_desc['MID_AT'] =  'Mid Atlantic US'\n",
    "    map_region_desc['NE'] = 'Northeastern US'\n",
    "else:\n",
    "    map_region_desc = dict()\n",
    "    map_region_desc['US'] = 'United States'\n",
    "\n",
    "df_regions = pd.DataFrame.from_dict(map_region_desc, orient='index').reset_index()\n",
    "df_regions.columns = ['regions', 'region_note']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "conn = sqlite3.connect(outputdB)\n",
    "c = conn.cursor()\n",
    "\n",
    "if remove_elec==1:\n",
    "    query = \"DELETE FROM Efficiency WHERE output_comm='ELCP' OR output_comm LIKE 'ELCP_Renewables%' \\\n",
    "    OR output_comm='ELCDIST_R' OR tech LIKE 'E_Batt%' OR tech LIKE 'E_TRANS%'\"\n",
    "    c.execute(query)\n",
    "    #query = \"DELETE FROM Efficiency WHERE input_comm='ELCP' OR input_comm='ELCP_Renewables'\"\n",
    "    #c.execute(query)\n",
    "    query = \"DELETE FROM tech_ramping\"\n",
    "    c.execute(query)\n",
    "    query = \"DELETE FROM RampUp\"\n",
    "    c.execute(query)\n",
    "    query = \"DELETE FROM RampDown\"\n",
    "    c.execute(query)\n",
    "    query = \"DELETE FROM MinGenGroupTarget\"\n",
    "    c.execute(query)\n",
    "\n",
    "if elec_only==1:\n",
    "    elec_demand = pd.read_csv(file_prefix + 'elec_demand.csv')\n",
    "    demands = elec_demand.groupby('periods').sum().reset_index()\n",
    "    demands = demands.melt(id_vars='periods')\n",
    "    demands.rename(columns={'region':'regions', 'value':'demand', 'variable':'regions'}, inplace=True)\n",
    "    demands['periods'] = demands['periods'].astype(int)\n",
    "    demands.insert(2,'demand_comm', 'ELC_dem')\n",
    "    demands.loc[:,'demand'] *=3.6e-6 #convert MWh to PJ\n",
    "    demands.loc[:,'demand_units'] = 'PJ'\n",
    "    demands.loc[:,'demand_notes'] = 'from PowerGenome'\n",
    "    df_table = demands[['regions','periods','demand_comm','demand','demand_units','demand_notes']]\n",
    "    df_table.to_sql('Demand',conn, if_exists='append', index=False)\n",
    "    \n",
    "    df_elc_comm = pd.DataFrame(columns=['comm_name', 'flag','comm_desc'], data=[['ELC_dem','d','Final electricity load']])\n",
    "    df_elc_comm.to_sql('commodities',conn, if_exists='append', index=False)\n",
    "    \n",
    "    df_elc_tech = pd.DataFrame(columns=['tech', 'flag','sector','tech_desc','tech_category'], data=[['ELC_BLND','p', 'electric','dummy technology to meet electricity demand commodity', '']])\n",
    "    df_elc_tech.to_sql('technologies',conn, if_exists='append', index=False)\n",
    "    \n",
    "    df_elc_tech = pd.DataFrame(columns=['regions', 'input_comm', 'tech', 'vintage','output_comm', 'efficiency', 'eff_notes'])\n",
    "    df_elc_tech.loc[:,'regions'] = df_regions['regions'].unique()\n",
    "    df_elc_tech.loc[:,'input_comm'] = 'ELC'\n",
    "    df_elc_tech.loc[:,'tech'] = 'ELC_BLND'\n",
    "    df_elc_tech.loc[:,'output_comm'] = 'ELC_dem'\n",
    "    df_elc_tech.loc[:,'efficiency'] = 1\n",
    "    df_elc_tech.loc[:,'vintage'] = start_year\n",
    "    df_elc_tech.loc[:,'eff_notes'] = 'dummy technology to meet electricity demand commodity'\n",
    "    df_elc_tech.to_sql('Efficiency',conn, if_exists='append', index=False)\n",
    "    \n",
    "    query = \"UPDATE Efficiency SET output_comm='ELCP_Renewables' WHERE tech LIKE 'E_SOLPVENDUSE%'\"\n",
    "    c.execute(query)\n",
    "    query = \"UPDATE MinActivity set regions='global' WHERE regions='US'\"\n",
    "    c.execute(query)\n",
    "    query = \"UPDATE MaxActivity set regions='global' WHERE regions='US'\"\n",
    "    c.execute(query)\n",
    "    \n",
    "    for table in table_list:\n",
    "        df_cols = c.execute(\"SELECT * FROM pragma_table_info('\" + table[0] + \"')\").fetchall()\n",
    "        df_cols = [x[1] for x in df_cols]\n",
    "\n",
    "        if table[0]=='regions':\n",
    "            query = \"DELETE FROM \" + table[0]\n",
    "            c.execute(query)\n",
    "            model_regions_df = df_regions[df_regions['regions'].isin(model_regions)].reset_index(drop=True)\n",
    "            model_regions_df.to_sql(table[0],conn, if_exists='append', index=False)\n",
    "        elif 'regions' in df_cols:\n",
    "            df_remains = pd.read_sql_query(\"SELECT * FROM \" + table[0] + \" WHERE regions='US'\", conn)\n",
    "#             query = \"DELETE FROM \" + table[0] + \" WHERE regions='US'\"\n",
    "            query = \"DELETE FROM \" + table[0] + \" WHERE regions NOT IN ('\" + \"','\".join(model_regions) + \"')\"\n",
    "            c.execute(query)\n",
    "            for reg in model_regions:\n",
    "                df_remains_new = df_remains.copy()\n",
    "                df_remains_new.regions=reg\n",
    "                if table[0] in ['ExistingCapacity']:\n",
    "                    df_remains_new['exist_cap'] /=9\n",
    "#                 if table[0] in ['MinActivity']:\n",
    "#                     df_remains_new['minact'] /=20\n",
    "                try:\n",
    "                    df_remains_new.to_sql(table[0],conn, if_exists='append', index=False)\n",
    "                except:\n",
    "                    pass\n",
    "        elif 'primary_region' in df_cols:\n",
    "            df_remains = pd.read_sql_query(\"SELECT * FROM \" + table[0] + \" WHERE primary_region='US'\", conn)\n",
    "            query = \"DELETE FROM \" + table[0] + \" WHERE primary_region='US'\"\n",
    "            c.execute(query)\n",
    "            for reg in model_regions:\n",
    "                df_remains_new = df_remains.copy()\n",
    "                df_remains_new['primary_region']=reg\n",
    "                try:\n",
    "                    df_remains_new.to_sql(table[0],conn, if_exists='append', index=False)\n",
    "                except:\n",
    "                    pass\n",
    "\n",
    "iterval = 0\n",
    "while len(c.execute(\"SELECT * FROM Efficiency WHERE output_comm NOT IN (SELECT input_comm FROM Efficiency)\\\n",
    "                       AND output_comm NOT IN (SELECT comm_name FROM commodities WHERE flag='d');\").fetchall()) > 0:\n",
    "\n",
    "    c.execute(\"DELETE FROM Efficiency WHERE output_comm NOT IN (SELECT input_comm FROM Efficiency) \\\n",
    "             AND output_comm NOT IN (SELECT comm_name FROM commodities WHERE flag='d');\")\n",
    "    iterval+=1\n",
    "    if iterval>10:\n",
    "        break\n",
    "print(iterval)\n",
    "\n",
    "for table in table_list:\n",
    "    if table[0] != 'Efficiency':  \n",
    "        df_cols = c.execute(\"SELECT * FROM pragma_table_info('\" + table[0] + \"')\").fetchall()\n",
    "        df_cols = [x[1] for x in df_cols]\n",
    "        if 'tech' in df_cols:\n",
    "            c.execute(\"UPDATE \"+str(table[0])+\" SET tech = TRIM(tech, CHAR(37,13,10));\")\n",
    "            try:\n",
    "                if 'vintage' in df_cols:\n",
    "                    # If t doesn't exist in Efficiency table after the deletions made above, \n",
    "                    # it is deleted from other tables.\n",
    "                    c.execute(\"DELETE FROM \"+str(table[0])+\" WHERE tech || vintage \\\n",
    "                        NOT IN (SELECT tech || vintage FROM Efficiency)\")\n",
    "                else:\n",
    "                    c.execute(\"DELETE FROM \"+str(table[0])+\" WHERE tech NOT IN (SELECT tech FROM Efficiency);\")\n",
    "            except:\n",
    "                print(table[0])\n",
    "        if table[0] == 'EmissionActivity':\n",
    "            c.execute(\"DELETE FROM EmissionActivity WHERE input_comm || tech || vintage || output_comm \\\n",
    "                        NOT IN (SELECT input_comm || tech || vintage || output_comm FROM Efficiency)\")\n",
    "\n",
    "\n",
    "c.execute(\"UPDATE commodities SET comm_name = TRIM(comm_name, CHAR(10,13,37))\")\n",
    "    # delete unused commodities otherwise the model throws an error\n",
    "c.execute(\"DELETE FROM commodities WHERE flag!='e' AND comm_name NOT IN (SELECT input_comm from Efficiency UNION SELECT output_comm from Efficiency);\")\n",
    "\n",
    "#update buildings sector\n",
    "if elec_only!=1:\n",
    "    #delete unused input_comm/technology rows from the efficiency and techinput split table\n",
    "    df_buildings_eff_del = pd.read_csv('buildings/TablesForDB/efficiency_buildings_delete.csv')\n",
    "    for ind, row in df_buildings_eff_del.iterrows():\n",
    "        query = \"DELETE FROM Efficiency WHERE tech = '\" + row['tech'] + \"' AND input_comm = '\" +\\\n",
    "        row['input_comm'] + \"'\"\n",
    "        c.execute(query)\n",
    "        query = \"DELETE FROM techinputsplit WHERE tech = '\" + row['tech'] + \"' AND input_comm = '\" +\\\n",
    "        row['input_comm'] + \"'\"\n",
    "        c.execute(query)\n",
    "\n",
    "    #update regional building demands \n",
    "    df_buildings = pd.read_csv('buildings/TablesForDB/annualDemand_buildings_regional.csv')\n",
    "    df_buildings = df_buildings[['regions', 'periods', 'demand_comm', 'demand','demand_units']]\n",
    "    df_buildings['demand_notes'] = ''\n",
    "    new_dems = []#['RCW', 'RCD']#, 'RFZ', 'ROTHER']'RCK', , 'RDW', \n",
    "    df_buildings = df_buildings[~df_buildings['demand_comm'].isin(new_dems)]\n",
    "#     df_buildings.loc[df_buildings['demand_comm']=='RCD','demand'] = 0\n",
    "#     df_buildings.loc[df_buildings['demand_comm']=='RDW','demand'] = 0\n",
    "#     df_buildings.loc[df_buildings['demand_comm']=='RCK','demand'] = 0\n",
    "#     df_buildings.loc[df_buildings['demand_comm']=='RCW','demand'] = 0\n",
    "#     df_buildings.loc[df_buildings['demand_comm']=='RFZ','demand'] = 0\n",
    "#     df_buildings.loc[df_buildings['demand_comm']=='ROTHER','demand'] = 0\n",
    "    df_buildings = df_buildings[df_buildings['periods'].isin(all_periods)]\n",
    "    if single_region==1:\n",
    "        df_buildings = df_buildings.groupby(by=['periods', 'demand_comm','demand_units','demand_notes']).sum().reset_index()\n",
    "        df_buildings.insert(0,'regions','US')\n",
    "        df_buildings = df_buildings[['regions', 'periods', 'demand_comm', 'demand','demand_units','demand_notes']]\n",
    "    old_demands = pd.read_sql_query(\"SELECT DISTINCT(demand_comm) FROM Demand WHERE substr(demand_comm,1,1) = 'C'OR substr(demand_comm,1,1) = 'R'\", conn)\n",
    "    query = \"DELETE FROM Demand WHERE \\\n",
    "    substr(demand_comm,1,1) = 'C'OR substr(demand_comm,1,1) = 'R'\"\n",
    "    c.execute(query)\n",
    "    df_buildings.drop_duplicates(subset=['regions','periods','demand_comm'], inplace=True)\n",
    "    df_buildings.to_sql('Demand',conn, if_exists='append', index=False) \n",
    "    \n",
    "    delete_demands = set([x[0] for x in old_demands.values]).difference(set(df_buildings['demand_comm'].unique()))\n",
    "    query = 'DELETE FROM DemandSpecificDistribution WHERE demand_name IN (\"' + '\", \"'.join(delete_demands) + '\")'\n",
    "    c.execute(query)\n",
    "    \n",
    "    #add new demands to commodities table\n",
    "    query = 'SELECT comm_name FROM commodities WHERE flag=\"d\"'\n",
    "    df_exist_demand = pd.read_sql_query(query, conn)\n",
    "    df_demand_comm = pd.DataFrame(columns=['comm_name','flag','comm_desc'])\n",
    "    df_demand_comm.loc[:,'comm_name'] = list(set(df_buildings.loc[:,'demand_comm'].unique()) - set(df_exist_demand['comm_name']))\n",
    "    df_demand_comm.loc[:,'flag'] = 'd'\n",
    "    df_demand_comm.loc[:,'comm_desc'] = 'residential demand'\n",
    "    df_demand_comm.to_sql('commodities',conn, if_exists='append', index=False)\n",
    "    \n",
    "    #update regional building efficiencies\n",
    "    df_buildings_eff_all = pd.read_csv('buildings/TablesForDB/efficiency_buildings_regional.csv')\n",
    "    df_buildings_eff = df_buildings_eff_all[['regions', 'input_comm','tech','vintage','output_comm', 'efficiency', 'eff_notes']]\n",
    "    df_buildings_eff = df_buildings_eff[~df_buildings_eff['tech'].str.contains('R_LT_LED60HE_GSL_N')]\n",
    "    df_buildings_eff = df_buildings_eff[~df_buildings_eff['tech'].str.contains('R_LT_LED60ST_GSL_N')]\n",
    "    df_buildings_eff.loc[df_buildings_eff['efficiency']=='#VALUE!','efficiency'] = 1\n",
    "    df_buildings_eff['efficiency'] = pd.to_numeric(df_buildings_eff['efficiency'])\n",
    "    df_buildings_eff['tech'] = df_buildings_eff['tech'].str.strip()\n",
    "    df_buildings_eff.drop_duplicates(subset=['regions', 'input_comm','tech','vintage','output_comm'],inplace=True)\n",
    "    if single_region==1:\n",
    "        df_buildings_eff = df_buildings_eff.groupby(by=['input_comm','tech','vintage','output_comm', 'eff_notes']).mean().reset_index()\n",
    "        df_buildings_eff.insert(0,'regions','US')\n",
    "        df_buildings_eff = df_buildings_eff[['regions', 'input_comm','tech','vintage','output_comm', 'efficiency', 'eff_notes']]\n",
    "    query = \"DELETE FROM Efficiency WHERE \\\n",
    "    (substr(output_comm,1,1) = 'C'OR substr(output_comm,1,1) = 'R') AND \\\n",
    "    (output_comm IN (SELECT DISTINCT(comm_name) FROM commodities WHERE flag='d')) AND (tech NOT LIKE '%BLND%')\"\n",
    "    c.execute(query)\n",
    "    query = 'DELETE FROM Efficiency WHERE tech IN (\"' + '\", \"'.join(df_buildings_eff['tech'].unique()) + '\")'\n",
    "    c.execute(query)\n",
    "    query = 'DELETE FROM Efficiency WHERE output_comm IN (\"' + '\", \"'.join(delete_demands) + '\")'\n",
    "    c.execute(query)\n",
    "    query = 'DELETE FROM Efficiency WHERE output_comm IN (\"' + '\", \"'.join(df_buildings_eff['output_comm'].unique()) + '\") \\\n",
    "    AND output_comm NOT IN (SELECT DISTINCT(demand_comm) FROM Demand)'\n",
    "    c.execute(query)\n",
    "    df_buildings_eff = df_buildings_eff[~df_buildings_eff['output_comm'].isin(new_dems)] #FIX\n",
    "    df_buildings_eff.to_sql('Efficiency',conn, if_exists='append', index=False)\n",
    "            \n",
    "    iterval = 0\n",
    "    while len(c.execute(\"SELECT * FROM Efficiency WHERE output_comm NOT IN (SELECT input_comm FROM Efficiency)\\\n",
    "                           AND output_comm NOT IN (SELECT comm_name FROM commodities WHERE flag='d');\").fetchall()) > 0:\n",
    "\n",
    "        c.execute(\"DELETE FROM Efficiency WHERE output_comm NOT IN (SELECT input_comm FROM Efficiency) \\\n",
    "                 AND output_comm NOT IN (SELECT comm_name FROM commodities WHERE flag='d');\")\n",
    "        iterval+=1\n",
    "        if iterval>10:\n",
    "            break\n",
    "    print(iterval)\n",
    "    \n",
    "    #add techinputsplit\n",
    "    query = 'DELETE FROM TechInputSplit WHERE tech NOT IN (SELECT DISTINCT(tech) FROM Efficiency)'\n",
    "    c.execute(query)\n",
    "    #remove cooking service demand splits that constrain NG use for cooking in US_BASE\n",
    "    query = 'DELETE FROM TechInputSplit WHERE tech=\"C_BLND_FUEL_CK\"'\n",
    "    c.execute(query)\n",
    "    #remove cooling service demand splits that constrain cooling type in US_BASE\n",
    "    query = 'DELETE FROM TechInputSplit WHERE tech=\"C_BLND_FUEL_SC\"'\n",
    "    c.execute(query)\n",
    "    #remove cooling service demand splits that constrain cooling type in US_BASE\n",
    "    query = 'DELETE FROM TechInputSplit WHERE tech=\"R_BLND_FUEL_SC\"'\n",
    "    c.execute(query)\n",
    "    \n",
    "    df_buildings_tisplit = pd.read_csv('buildings/TablesForDB/tech_input_split.csv', index_col=0)\n",
    "    df_buildings_tisplit['ti_split_notes'].fillna('', inplace=True)\n",
    "    df_buildings_tisplit = df_buildings_tisplit[df_buildings_tisplit['periods']>=2020]\n",
    "    existing_techs_tisplit = df_buildings_tisplit['tech'].unique()\n",
    "    \n",
    "    query = 'DELETE FROM TechInputSplit WHERE tech IN (\"' + '\", \"'.join(existing_techs_tisplit) + '\")'\n",
    "    c.execute(query)\n",
    "\n",
    "    if single_region==1:\n",
    "        df_buildings_tisplit = df_buildings_tisplit[df_buildings_tisplit['regions']=='US']\n",
    "    else:\n",
    "        df_buildings_tisplit = df_buildings_tisplit[df_buildings_tisplit['regions']!='US']\n",
    "#         df_buildings_tisplit = df_buildings_tisplit.groupby(by=['periods','input_comm','tech', 'ti_split_notes']).mean().reset_index()\n",
    "#         df_buildings_tisplit.insert(0,'regions','US')\n",
    "#         df_buildings_tisplit = df_buildings_tisplit[['regions', 'periods','input_comm','tech', 'ti_split','ti_split_notes']]\n",
    "        \n",
    "    df_buildings_tisplit.to_sql('TechInputSplit',conn, if_exists='append', index=False)\n",
    "    \n",
    "    \n",
    "    #update buildings costs invest\n",
    "    df_buildings_invest_all = pd.read_csv('buildings/TablesForDB/costInvest_buildings_regional.csv')\n",
    "#     df_buildings_invest_all['cost_invest'] *= 10**6\n",
    "#     df_buildings_invest_all = df_buildings_invest_all[~df_buildings_invest_all['cost_invest'].isna()]\n",
    "    df_buildings_invest_all = df_buildings_invest_all[~df_buildings_invest_all['tech'].str.contains('R_LT_LED60HE_GSL_N')]\n",
    "    df_buildings_invest_all = df_buildings_invest_all[~df_buildings_invest_all['tech'].str.contains('R_LT_LED60ST_GSL_N')]\n",
    "    df_buildings_invest = df_buildings_invest_all[['regions','tech','vintage','cost_invest','cost_invest_units']]\n",
    "    df_buildings_invest['cost_invest_notes'] = df_buildings_invest_all['capacity'].map(str) + ' ' + df_buildings_invest_all['capacity units']\n",
    "    df_buildings_invest.drop_duplicates(subset=['regions', 'tech','vintage'], inplace=True)\n",
    "    if single_region==1:\n",
    "        df_buildings_invest = df_buildings_invest.groupby(by=['tech','vintage', 'cost_invest_units']).mean().reset_index()\n",
    "        df_buildings_invest.insert(0,'regions','US')\n",
    "        df_buildings_invest = df_buildings_invest[['regions','tech','vintage','cost_invest','cost_invest_units']]\n",
    "    query = 'DELETE FROM CostInvest WHERE tech IN (\"' + '\", \"'.join(df_buildings_invest['tech'].unique()) + '\")'\n",
    "    c.execute(query)\n",
    "    query = 'DELETE FROM CostInvest WHERE tech NOT IN (SELECT DISTINCT(tech) FROM Efficiency)'\n",
    "    c.execute(query)\n",
    "    df_buildings_invest.to_sql('CostInvest',conn, if_exists='append', index=False)\n",
    "    query = 'DELETE FROM CostInvest WHERE tech NOT IN (SELECT DISTINCT(tech) FROM Efficiency)' #FIX\n",
    "    c.execute(query) #FIX\n",
    "   \n",
    "    #update buildings costs fixed\n",
    "    df_buildings_fixed_all = pd.read_csv('buildings/TablesForDB/costFixed_buildings_regional.csv')\n",
    "    df_buildings_fixed_all = df_buildings_fixed_all[~df_buildings_fixed_all['tech'].str.contains('R_LT_LED60HE_GSL_N')]\n",
    "    df_buildings_fixed_all = df_buildings_fixed_all[~df_buildings_fixed_all['tech'].str.contains('R_LT_LED60ST_GSL_N')]\n",
    "    df_buildings_fixed = df_buildings_fixed_all[['regions','periods','tech','vintage','cost_fixed','cost_fixed_units']]\n",
    "    df_buildings_fixed['cost_fixed_notes'] = df_buildings_fixed_all['capacity'].map(str) + ' ' + df_buildings_fixed_all['capacity_units']\n",
    "    df_buildings_fixed.drop_duplicates(subset=['regions', 'periods','tech','vintage'], inplace=True)\n",
    "    df_buildings_fixed = df_buildings_fixed[df_buildings_fixed['periods'].isin(all_periods)]\n",
    "    if single_region==1:\n",
    "        df_buildings_fixed = df_buildings_fixed.groupby(by=['periods','tech','vintage','cost_fixed_units']).mean().reset_index()\n",
    "        df_buildings_fixed.insert(0,'regions','US')\n",
    "        df_buildings_fixed = df_buildings_fixed[['regions','periods','tech','vintage','cost_fixed','cost_fixed_units']]\n",
    "    query = 'DELETE FROM Costfixed WHERE tech IN (\"' + '\", \"'.join(df_buildings_fixed['tech'].unique()) + '\")'\n",
    "    c.execute(query)\n",
    "    df_buildings_fixed.to_sql('CostFixed',conn, if_exists='append', index=False)\n",
    "    query = 'DELETE FROM CostFixed WHERE tech NOT IN (SELECT DISTINCT(tech) FROM Efficiency)' #FIX\n",
    "    c.execute(query)\n",
    "    \n",
    "    #update variable costs for techs no longer needed for the buildings sector\n",
    "    query = 'DELETE FROM CostVariable WHERE tech NOT IN (SELECT DISTINCT(tech) FROM Efficiency)'\n",
    "    c.execute(query)\n",
    "    \n",
    "    #update EmissionActivity for techs no longer needed for the buildings sector\n",
    "    query = 'DELETE FROM EmissionActivity WHERE tech NOT IN (SELECT DISTINCT(tech) FROM Efficiency)'\n",
    "    c.execute(query)\n",
    "    \n",
    "    #update buildings lifetimes\n",
    "    df_buildings_lifetime = pd.read_csv('buildings/TablesForDB/lifetime_buildings_regional.csv')\n",
    "    df_buildings_lifetime = df_buildings_lifetime[~df_buildings_lifetime['tech'].str.contains('R_LT_LED60HE_GSL_N')]\n",
    "    df_buildings_lifetime = df_buildings_lifetime[~df_buildings_lifetime['tech'].str.contains('R_LT_LED60ST_GSL_N')]\n",
    "    df_buildings_lifetime = df_buildings_lifetime[['regions','tech','life', 'life_notes']]\n",
    "    df_buildings_lifetime.drop_duplicates(subset=['regions', 'tech'], inplace=True)\n",
    "    if single_region==1:\n",
    "        df_buildings_lifetime = df_buildings_lifetime.groupby(by=['tech','life_notes']).mean().reset_index()\n",
    "        df_buildings_lifetime.insert(0,'regions','US')\n",
    "        df_buildings_lifetime = df_buildings_lifetime[['regions','tech','life', 'life_notes']]\n",
    "    query = 'DELETE FROM LifetimeTech WHERE tech IN (\"' + '\", \"'.join(df_buildings_lifetime['tech'].unique()) + '\")'\n",
    "    c.execute(query)\n",
    "    query = 'DELETE FROM LifetimeTech WHERE tech NOT IN (SELECT DISTINCT(tech) FROM Efficiency)'\n",
    "    c.execute(query)\n",
    "    df_buildings_lifetime.to_sql('LifetimeTech',conn, if_exists='append', index=False)\n",
    "    query = 'DELETE FROM LifetimeTech WHERE tech NOT IN (SELECT DISTINCT(tech) FROM Efficiency)' #FIX\n",
    "    c.execute(query) #FIX\n",
    "    \n",
    "    #update buildings existing capacity\n",
    "    df_buildings_existcap = pd.concat([pd.read_csv('buildings/TablesForDB/residential_existingCapacity.csv'), \\\n",
    "                        pd.read_csv('buildings/TablesForDB/commercial_existingCapacity.csv')]) \n",
    "    df_buildings_existcap.rename(columns={'Region_OEO':'regions', 'ExistingCap':'exist_cap'}, inplace=True)\n",
    "    df_buildings_existcap = df_buildings_existcap[['regions','tech','vintage','exist_cap']]\n",
    "    df_buildings_existcap['exist_cap_units'] = ''\n",
    "    df_buildings_existcap['exist_cap_notes'] = ''\n",
    "    if single_region==1:\n",
    "        df_buildings_existcap = df_buildings_existcap.groupby(by=['tech','vintage', 'exist_cap_units', 'exist_cap_notes']).sum().reset_index()\n",
    "        df_buildings_existcap.insert(0,'regions','US')\n",
    "        df_buildings_existcap = df_buildings_existcap[['regions','tech','vintage','exist_cap', 'exist_cap_units', 'exist_cap_notes']]\n",
    "    query = 'DELETE FROM ExistingCapacity WHERE tech IN (\"' + '\", \"'.join(df_buildings_existcap['tech'].unique()) + '\")'\n",
    "    c.execute(query)\n",
    "    query = 'DELETE FROM ExistingCapacity WHERE tech NOT IN (SELECT DISTINCT(tech) FROM Efficiency)'\n",
    "    c.execute(query)\n",
    "    df_buildings_existcap.to_sql('ExistingCapacity',conn, if_exists='append', index=False)\n",
    "    query = 'DELETE FROM ExistingCapacity WHERE tech NOT IN (SELECT DISTINCT(tech) FROM Efficiency)'#FIX\n",
    "    c.execute(query) #FIX\n",
    "    \n",
    "    #add discount rate\n",
    "    df_buildings_discrate = pd.read_csv('buildings/TablesForDB/discount_rate.csv')   \n",
    "    df_buildings_discrate = df_buildings_discrate[~df_buildings_discrate['tech'].str.contains('R_LT_LED60HE_GSL_N')]\n",
    "    df_buildings_discrate = df_buildings_discrate[~df_buildings_discrate['tech'].str.contains('R_LT_LED60ST_GSL_N')]\n",
    "    df_buildings_discrate = df_buildings_discrate[['regions','tech','vintage','tech_rate', 'tech_rate_notes']]\n",
    "    df_buildings_discrate['tech_rate_notes'].fillna('N/A',inplace=True)\n",
    "\n",
    "    if single_region==1:\n",
    "        df_buildings_discrate = df_buildings_discrate.groupby(by=['tech','vintage', 'tech_rate_notes']).mean().reset_index()\n",
    "        df_buildings_discrate.insert(0,'regions','US')\n",
    "        df_buildings_discrate = df_buildings_discrate[['regions','tech','vintage','tech_rate', 'tech_rate_notes']]\n",
    "    query = 'DELETE FROM DiscountRate WHERE tech IN (\"' + '\", \"'.join(df_buildings_discrate['tech'].unique()) + '\")'\n",
    "    c.execute(query)\n",
    "    query = 'DELETE FROM DiscountRate WHERE tech NOT IN (SELECT DISTINCT(tech) FROM Efficiency)'\n",
    "    c.execute(query)\n",
    "    df_buildings_discrate.to_sql('DiscountRate',conn, if_exists='append', index=False)  \n",
    "    query = 'DELETE FROM DiscountRate WHERE tech NOT IN (SELECT DISTINCT(tech) FROM Efficiency)' #FIX\n",
    "    c.execute(query)#FIX\n",
    "    \n",
    "    #update technologies table\n",
    "    query = 'DELETE FROM technologies WHERE tech NOT in (SELECT DISTINCT(tech) FROM Efficiency)'\n",
    "    c.execute(query)\n",
    "    query = 'SELECT DISTINCT(tech) FROM Efficiency WHERE tech NOT IN (SELECT tech FROM technologies)'\n",
    "    tech_table = pd.read_sql_query(query, conn)\n",
    "    if len(tech_table)>0:\n",
    "        tech_table.loc[:, 'flag'] = 'p'\n",
    "        tech_table.loc[:, 'sector'] = 'residential'\n",
    "        mask = tech_table['tech'].str[0]=='C'\n",
    "        tech_table.loc[mask, 'sector'] = 'commercial'\n",
    "        tech_table.loc[:, 'tech_desc'] = ''\n",
    "        tech_table.loc[:, 'tech_category'] = ''\n",
    "        #add tech descriptions\n",
    "        tech_desc = df_buildings_eff_all[['tech', 'Tech Description']].drop_duplicates()\n",
    "        #drop duplicates for heat pump techs that have the same description, but different heating or cooling suffix \n",
    "        tech_desc['Tech Description'] = [x.split('  ')[0].strip() if 'HEAT PUMP' in x else x for x in tech_desc['Tech Description'] ]\n",
    "        tech_desc['Tech Description'] = [x.split(' -')[0].strip() if 'HEAT PUMP' in x else x for x in tech_desc['Tech Description'] ]\n",
    "        tech_desc['Tech Description'] = [x.split(' FOR SPACE')[0].strip() if 'HEAT PUMP' in x else x for x in tech_desc['Tech Description'] ]\n",
    "        tech_desc = tech_desc[['tech', 'Tech Description']].drop_duplicates()\n",
    "        tech_desc['tech_desc'] = tech_desc['Tech Description'].str.lower()\n",
    "        tech_table = tech_table.drop(columns='tech_desc').merge(tech_desc.drop(columns='Tech Description'), on = 'tech', how='left')\n",
    "        tech_table = tech_table[['tech', 'flag','sector','tech_desc','tech_category']]\n",
    "        tech_table.to_sql('technologies',conn, if_exists='append', index=False)\n",
    "        \n",
    "    #update commodities table\n",
    "    query = 'DELETE FROM commodities WHERE comm_name IN (\"' + '\", \"'.join(delete_demands) + '\")'\n",
    "    c.execute(query)\n",
    "    query = 'DELETE FROM commodities WHERE comm_name NOT in (SELECT DISTINCT(input_comm) FROM Efficiency)\\\n",
    "    AND comm_name NOT in (SELECT DISTINCT(output_comm) FROM Efficiency) AND (flag=\"p\" OR flag=\"d\") '\n",
    "    c.execute(query)\n",
    "    query = 'SELECT DISTINCT(input_comm) FROM Efficiency WHERE input_comm NOT IN (SELECT comm_name FROM commodities)'\n",
    "    tech_table = pd.read_sql_query(query, conn)\n",
    "    if len(tech_table)>0:\n",
    "        tech_table.loc[:, 'flag'] = 'p'\n",
    "        tech_table.loc[:, 'comm_desc'] = ''\n",
    "        tech_table.rename(columns={'input_comm':'comm_name'}, inplace=True)\n",
    "        tech_table.to_sql('commodities',conn, if_exists='append', index=False)\n",
    "    query = 'SELECT DISTINCT(output_comm) FROM Efficiency WHERE output_comm NOT IN (SELECT comm_name FROM commodities)'\n",
    "    tech_table = pd.read_sql_query(query, conn)\n",
    "    if len(tech_table)>0:\n",
    "        tech_table.loc[:, 'flag'] = 'p'\n",
    "        tech_table.loc[:, 'comm_desc'] = ''\n",
    "        tech_table.rename(columns={'output_comm':'comm_name'}, inplace=True)\n",
    "        tech_table.to_sql('commodities',conn, if_exists='append', index=False) \n",
    "\n",
    "if elec_only!=1 :\n",
    "# if (single_region!=1) & (elec_only!=1) :\n",
    "    #update regional transportation demands \n",
    "    transp_file = 'transport/transport_regionalization_master.xlsx'\n",
    "    transp_sheets = pd.ExcelFile(transp_file)\n",
    "    df_trans_del = pd.read_excel(transp_file, sheet_name='needs deleting', header=None)\n",
    "    for sheet in transp_sheets.sheet_names:\n",
    "        if sheet!='needs deleting':\n",
    "            df_trans = pd.read_excel(transp_file, sheet_name=sheet)\n",
    "            if sheet=='ExistingCapacity':\n",
    "                df_trans = df_trans[df_trans['exist_cap']>0]\n",
    "                df_trans['exist_cap'] *= 2/3 #adjust so that only 2/3 capacity survives to 2020\n",
    "            if sheet=='Demand':\n",
    "                query = 'DELETE FROM Demand WHERE demand_comm IN (\"' + '\", \"'.join(df_trans['demand_comm'].unique()) + '\")'\n",
    "                c.execute(query)\n",
    "                df_trans.to_sql(sheet,conn, if_exists='append', index=False)\n",
    "#             elif 'vintage' in df_trans.columns: \n",
    "#                 df_trans['query'] = 'tech = \"' + df_trans['tech'] + '\" AND vintage = ' + df_trans['vintage'].map(str)\n",
    "#                 query = 'DELETE FROM ' + sheet + ' WHERE (' + ') OR ('.join(df_trans['query']) + ')'\n",
    "#                 if sheet=='Efficiency':\n",
    "#                     print(query)\n",
    "#                     break\n",
    "#                 c.execute(query)\n",
    "#                 df_trans.drop(columns='query').to_sql(sheet,conn, if_exists='append', index=False)\n",
    "#             elif sheet=='TechInputSplit':\n",
    "#                 query = 'DELETE FROM ' + sheet + ' WHERE tech IN (\"' + '\", \"'.join(df_trans['tech'].unique()) + '\")'\n",
    "#                 c.execute(query)\n",
    "#                 df_trans.to_sql(sheet,conn, if_exists='append', index=False)\n",
    "#             elif 'regions' in df_trans.columns:\n",
    "#                 df_trans.to_sql(sheet,conn, if_exists='append', index=False)\n",
    "            else:\n",
    "                query = 'DELETE FROM ' + sheet + ' WHERE tech IN (\"' + '\", \"'.join(df_trans['tech'].unique()) + '\")'\n",
    "                c.execute(query)\n",
    "\n",
    "                df_trans.to_sql(sheet,conn, if_exists='append', index=False)\n",
    "\n",
    "    for table in table_list:\n",
    "        df_cols = c.execute(\"SELECT * FROM pragma_table_info('\" + table[0] + \"')\").fetchall()\n",
    "        df_cols = [x[1] for x in df_cols]\n",
    "        if 'tech' in df_cols:\n",
    "            query = 'DELETE FROM ' + table[0] + ' WHERE tech IN (\"' + '\", \"'.join(df_trans_del[0].unique()) + '\")'\n",
    "            c.execute(query)\n",
    "    \n",
    "    #add fuels transport\n",
    "    transport_fuels = ['ETH_CORN', 'ETH_CEL']#['GAS', 'E10']\n",
    "    df_transp_fuels = pd.DataFrame(columns=['regions','input_comm','tech','vintage','output_comm','efficiency', 'eff_notes'])\n",
    "    df_transp_fuels_regs = []\n",
    "    for i in combinations(regions_list,2):\n",
    "        df_transp_fuels_regs.append('-'.join(i))\n",
    "    df_transp_fuels['regions'] = df_transp_fuels_regs\n",
    "    df_transp_fuels['vintage'] = start_year\n",
    "    df_transp_fuels['efficiency'] = 0.99\n",
    "    df_transp_fuels_swap = df_transp_fuels.copy()\n",
    "    df_transp_fuels_swap['regions'] = df_transp_fuels['regions'].apply(swap_regions)\n",
    "    df_transp_fuels = pd.concat([df_transp_fuels, df_transp_fuels_swap])\n",
    "    for fu in transport_fuels:\n",
    "        df_transp_fuels['input_comm'] = fu\n",
    "        df_transp_fuels['output_comm'] = fu\n",
    "        df_transp_fuels['tech'] = fu + '_TRANS'\n",
    "        df_transp_fuels.to_sql('Efficiency',conn, if_exists='append', index=False)\n",
    "        tech_exchange = [fu + '_TRANS', fu + ' inter-region transport']\n",
    "        tech_exchange = pd.DataFrame(data=[tech_exchange], columns=['tech', 'notes'])\n",
    "        tech_exchange.to_sql('tech_exchange',conn, if_exists='append', index=False)\n",
    "#         if fu!='GAS':\n",
    "#             tech_exchange.to_sql('tech_annual',conn, if_exists='append', index=False)\n",
    "        techn = [fu + '_TRANS', 'p', 'transport', fu + ' inter-region transport', '']\n",
    "        techn = pd.DataFrame(data=[techn], columns=['tech', 'flag', 'sector', 'tech_desc', 'tech_category'])\n",
    "        techn.to_sql('technologies',conn, if_exists='append', index=False)\n",
    "\n",
    "    #update regional industrial sector\n",
    "#     df_industry = pd.read_csv('industry/industry_regional_demands_adjusted.csv')\n",
    "#     df_industry = df_industry[['regions', 'periods', 'demand_comm', 'demand','demand_units','demand_notes']]\n",
    "#     query = \"DELETE FROM Demand  \\\n",
    "#     WHERE demand_comm LIKE '%IND%'\"\n",
    "#     c.execute(query)\n",
    "#     df_industry.to_sql('Demand',conn, if_exists='append', index=False)\n",
    "    cwd = os.getcwd()\n",
    "    os.chdir('industry')\n",
    "    %run industry_integration_OEO.ipynb\n",
    "    os.chdir(cwd)\n",
    "\n",
    "    #find remaining demands at US level, divide by 9 to get demands in each region\n",
    "    df_remains = pd.read_sql_query(\"SELECT * FROM Demand WHERE regions='US'\", conn)\n",
    "    query = \"DELETE FROM Demand WHERE regions='US'\"\n",
    "    c.execute(query)\n",
    "    for reg in model_regions:\n",
    "        df_remains_new = df_remains.copy()\n",
    "        df_remains_new.regions=reg\n",
    "        df_remains_new['demand'] /=9\n",
    "        df_remains_new.to_sql('Demand',conn, if_exists='append', index=False)\n",
    "        \n",
    "    query = \"UPDATE MinActivity set regions='global' WHERE regions='US'\"\n",
    "    c.execute(query)\n",
    "    query = \"UPDATE MaxActivity set regions='global' WHERE regions='US'\"\n",
    "    c.execute(query)\n",
    "    \n",
    "    for table in table_list:\n",
    "        df_cols = c.execute(\"SELECT * FROM pragma_table_info('\" + table[0] + \"')\").fetchall()\n",
    "        df_cols = [x[1] for x in df_cols]\n",
    "\n",
    "        if table[0]=='regions':\n",
    "            query = \"DELETE FROM \" + table[0]\n",
    "            c.execute(query)\n",
    "            df_regions.to_sql(table[0],conn, if_exists='append', index=False)\n",
    "        elif 'regions' in df_cols:\n",
    "            df_remains = pd.read_sql_query(\"SELECT * FROM \" + table[0] + \" WHERE regions='US'\", conn)\n",
    "            query = \"DELETE FROM \" + table[0] + \" WHERE regions='US'\"\n",
    "            c.execute(query)\n",
    "            for reg in model_regions:\n",
    "                df_remains_new = df_remains.copy()\n",
    "                df_remains_new.regions=reg\n",
    "                if table[0] in ['ExistingCapacity']:\n",
    "                    df_remains_new['exist_cap'] /=9\n",
    "#                 if table[0] in ['MinActivity']:\n",
    "#                     df_remains_new['minact'] /=20\n",
    "                if table[0] != 'TechInputSplitAverage': #don't split any TechInputSplitAverage data             \n",
    "                    df_remains_new.to_sql(table[0],conn, if_exists='append', index=False)\n",
    "        elif 'primary_region' in df_cols:\n",
    "            df_remains = pd.read_sql_query(\"SELECT * FROM \" + table[0] + \" WHERE primary_region='US'\", conn)\n",
    "            query = \"DELETE FROM \" + table[0] + \" WHERE primary_region='US'\"\n",
    "            c.execute(query)\n",
    "            for reg in model_regions:\n",
    "                df_remains_new = df_remains.copy()\n",
    "                df_remains_new['primary_region']=reg\n",
    "                df_remains_new.to_sql(table[0],conn, if_exists='append', index=False)\n",
    "            \n",
    "    #fix issue where some existing transport techs were deleted in the regional db\n",
    "    for table in table_list:\n",
    "        df_cols = c.execute(\"SELECT * FROM pragma_table_info('\" + table[0] + \"')\").fetchall()\n",
    "        df_cols = [x[1] for x in df_cols]\n",
    "        if 'regions' in df_cols and 'tech' in df_cols and 'vintage' in df_cols:\n",
    "            c.execute(\"DELETE FROM \"+str(table[0])+\" WHERE regions || tech || vintage \\\n",
    "                        NOT IN (SELECT regions || tech || vintage FROM ExistingCapacity)\\\n",
    "                        AND vintage < \" + str(start_year))\n",
    "\n",
    "query = \"DELETE FROM LinkedTechs WHERE primary_tech NOT IN (SELECT tech FROM technologies)\"\n",
    "c.execute(query)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide_input": false
   },
   "outputs": [],
   "source": [
    "#run jupyter notebook that adds EIA planned coal plant retirements\n",
    "%run electric/Coal_Retirements.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#update electric sector so2 and Nox emisions limits\n",
    "df_emislimits = pd.read_excel('electric/regional_elec_emissions_limits.xlsx', sheet_name = 'Formatted for temoa')\n",
    "query = 'DELETE FROM EmissionLimit WHERE emis_comm IN (\"' + '\", \"'.join(df_emislimits['emis_comm'].unique()) + '\")'\n",
    "c.execute(query)\n",
    "df_emislimits.to_sql('EmissionLimit',conn, if_exists='append', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #update coal and ng electric sector emissions factors\n",
    "\n",
    "query = \"DELETE FROM EmissionActivity WHERE tech LIKE 'E_COALSTM%' AND emis_comm LIKE '%_ELC'\"\n",
    "c.execute(query)\n",
    "query = \"DELETE FROM EmissionActivity WHERE emis_comm LIKE '%_ELC' AND tech LIKE 'E_EA_COA%' and output_comm NOT LIKE '%_IGCC_N'\"\n",
    "c.execute(query)\n",
    "query = \"DELETE FROM EmissionActivity WHERE emis_comm LIKE '%_ELC' AND tech LIKE 'E_NGA%' AND vintage < \" + str(start_year)\n",
    "c.execute(query)\n",
    "\n",
    "df_emisact = pd.read_csv('electric/Regional_EAs.csv')\n",
    "df_emisact = df_emisact.merge(df_efficiency[['regions','input_comm','tech', 'vintage', 'output_comm']], left_on = ['Region', 'input_com'], right_on = ['regions', 'input_comm'], how='left')\n",
    "df_emisact = df_emisact[df_emisact['vintage']<start_year]\n",
    "df_emisact = df_emisact[['regions','emis_comm','input_comm', 'tech', 'vintage', 'output_comm', 'emis_act', 'emis_act_units', 'emis_act_notes']]\n",
    "df_emisact['tech'] = df_emisact['tech'].str.replace('E_COALSTM_N_', 'E_COALSTM_R_')\n",
    "\n",
    "df_emisact.to_sql('EmissionActivity', conn, if_exists='append', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#add regional co2 sequestration limits, annual and total\n",
    "df_co2_seq_maxresource = pd.read_excel('supply/co2_sequestration/co2_sequestration_regional.xlsx', sheet_name='MaxResource')\n",
    "df_co2_seq_maxresource['maxres'] = np.round(df_co2_seq_maxresource['maxres'],0)\n",
    "\n",
    "query = \"DELETE FROM MaxResource WHERE tech in ('\" + \"','\".join(df_co2_seq_maxresource['tech'].unique()) + \"')\"\n",
    "c.execute(query)\n",
    "df_co2_seq_maxresource = df_co2_seq_maxresource[df_co2_seq_maxresource['regions'].isin(model_regions)]\n",
    "df_co2_seq_maxresource.to_sql('MaxResource', conn, if_exists='append', index=False)\n",
    "\n",
    "df_co2_seq_maxactivity = pd.read_excel('supply/co2_sequestration/co2_sequestration_regional.xlsx', sheet_name='MaxActivity')\n",
    "df_co2_seq_maxactivity['maxact'] = np.round(df_co2_seq_maxactivity['maxact'],0)\n",
    "\n",
    "query = \"DELETE FROM MaxActivity WHERE tech in ('\" + \"','\".join(df_co2_seq_maxactivity['tech'].unique()) + \"')\"\n",
    "c.execute(query)\n",
    "df_co2_seq_maxactivity = df_co2_seq_maxactivity[df_co2_seq_maxactivity['regions'].isin(model_regions)]\n",
    "df_co2_seq_maxactivity.to_sql('MaxActivity', conn, if_exists='append', index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#add co2_elc commodity to all electric sector technologies (use phase only, not upstream)\n",
    "query = \"SELECT * FROM EmissionActivity WHERE emis_comm='co2' \\\n",
    "AND tech IN (SELECT tech FROM technologies WHERE sector LIKE 'electric%')\"\n",
    "df_co2_elc = pd.read_sql_query(query, conn)\n",
    "df_co2_elc['emis_comm'] = 'co2_ELC'\n",
    "df_co2_elc.to_sql('EmissionActivity', conn, if_exists='append', index=False)\n",
    "\n",
    "df_elc_comm = pd.DataFrame(columns=['comm_name', 'flag','comm_desc'], data=[['co2_ELC','e','CO2 emissions from the electric sector']])\n",
    "df_elc_comm.to_sql('commodities',conn, if_exists='append', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#run jupyter notebook that adds biomass supply curve\n",
    "cwd = os.getcwd()\n",
    "os.chdir('supply/biomass/')\n",
    "%run BiomassSupplyCurves.ipynb\n",
    "os.chdir(cwd)\n",
    "\n",
    "if elec_only==1:\n",
    "    query = \"DELETE FROM Efficiency WHERE regions NOT IN ('\" + \"','\".join(model_regions) + \"')\"\n",
    "    c.execute(query)\n",
    "    query = \"DELETE FROM Efficiency WHERE output_comm IN ('SOY','CORN','STV')\"\n",
    "    c.execute(query)\n",
    "    query = \"DELETE FROM technologies WHERE tech NOT IN (SELECT DISTINCT tech FROM Efficiency)\"\n",
    "    c.execute(query)\n",
    "    query = \"DELETE FROM MaxActivity WHERE tech NOT IN (SELECT DISTINCT tech FROM Efficiency)\"\n",
    "    c.execute(query)\n",
    "    query = \"DELETE FROM CostVariable WHERE tech NOT IN (SELECT DISTINCT tech FROM Efficiency)\"\n",
    "    c.execute(query)\n",
    "    query = \"DELETE FROM commodities WHERE comm_name IN ('SOY','CORN','STV')\"\n",
    "    c.execute(query)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#incorporate California cap and trade emissions limits\n",
    "if (df_regions['regions']=='CA').any() & elec_only==0:\n",
    "    df_ca_cap = pd.read_excel('policies/CA_capandtrade_approximation.xlsx')\n",
    "    emissions_cap = df_ca_cap[df_ca_cap['Field']=='2030 energy sector emissions target']['Value']*1000 #ktons\n",
    "    emissions_cap_df = pd.DataFrame(columns=['regions','periods','emis_comm','emis_limit', 'emis_limit_units','emis_limit_notes'])\n",
    "    emissions_cap_df['periods'] = np.arange(2030, 2051, 5)\n",
    "    emissions_cap_df['regions']= 'CA'\n",
    "    emissions_cap_df['emis_comm']= 'co2'\n",
    "    emissions_cap_df['emis_limit']= emissions_cap.iloc[0]\n",
    "    emissions_cap_df['emis_limit_units']= 'kt'\n",
    "    emissions_cap_df['emis_limit_notes']= '40% below 1990 levels, assuming 86% of CA emissions from energy, ignoring imports'\n",
    "    emissions_cap_df.to_sql('EmissionLimit',conn, if_exists='append', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#normalize FOM to VOM for coal and NGCC assuming 65% capacity factor\n",
    "# df_FOM = pd.read_sql_query(\"SELECT * FROM CostFixed WHERE tech LIKE 'E_COALSTM_R%' OR tech LIKE 'E_NGACC_R%'\", conn)\n",
    "# df_VOM = pd.read_sql_query(\"SELECT * FROM CostVariable WHERE tech LIKE 'E_COALSTM_R%' OR tech LIKE 'E_NGACC_R%'\",conn)\n",
    "# df_FOM['cost_fixed']*=10**6/1000/(8760*0.65)\n",
    "# df_VOM = df_VOM.merge(df_FOM[['regions','periods','tech','vintage','cost_fixed']])\n",
    "# df_VOM['cost_variable'] +=df_VOM['cost_fixed']\n",
    "\n",
    "# c.execute(\"DELETE FROM CostFixed WHERE tech LIKE 'E_COALSTM_R%' OR tech LIKE 'E_NGACC_R%'\")\n",
    "# c.execute(\"DELETE FROM CostVariable WHERE tech LIKE 'E_COALSTM_R%' OR tech LIKE 'E_NGACC_R%'\")\n",
    "# df_VOM.drop(columns=['cost_fixed']).to_sql('CostVariable',conn, if_exists='append', index=False, method='multi', chunksize=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if elec_only==1:\n",
    "    for table in table_list:\n",
    "        df_cols = c.execute(\"SELECT * FROM pragma_table_info('\" + table[0] + \"')\").fetchall()\n",
    "        df_cols = [x[1] for x in df_cols]\n",
    "        if 'regions' in df_cols:\n",
    "            query = \"DELETE FROM \" + table[0] + \" WHERE regions NOT IN ('\" + \"','\".join(model_regions) + \"' AND regions NOT LIKE '%-%')\"\n",
    "            c.execute(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#add wind integration costs\n",
    "#https://www.energy.gov/sites/default/files/2021-08/Land-Based%20Wind%20Market%20Report%202021%20Edition_Full%20Report_FINAL.pdf\n",
    "# wind_intg_costs = np.round(5/3600*1e9/1e6,2) #$5/MWh to $M/PJ\n",
    "\n",
    "# df_intg_variable = df_costfixed_z[df_costfixed_z['tech'].str.contains('E_WND_N')].copy()\n",
    "# df_intg_variable.rename(columns={'cost_fixed':'cost_variable', 'cost_fixed_units':'cost_variable_units', \\\n",
    "#                                  'cost_fixed_notes':'cost_variable_notes'}, inplace=True)\n",
    "# df_intg_variable.loc[:, 'cost_variable_units'] = '$M/PJ'\n",
    "# df_intg_variable.loc[:, 'cost_variable_notes'] = '$5/MWh integration costs, Based on Land-Based Wind Market Report: 2021 Edition'\n",
    "# df_intg_variable.loc[:, 'cost_variable'] = wind_intg_costs\n",
    "# df_intg_variable.to_sql('CostVariable', conn, if_exists='append', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#include hydrogen inter-regional transport pipeline data\n",
    "cwd = os.getcwd()\n",
    "os.chdir('supply/hydrogen/')\n",
    "%run integrate_hydrogen_regional_transport.ipynb\n",
    "os.chdir(cwd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#update all sectors regional fuel prices\n",
    "df_fuelprices = pd.read_csv('supply/AEO_fuel_prices/AEO_2022/reference_case_regional_fuel_prices.csv')\n",
    "query = 'DELETE FROM CostVariable WHERE tech IN (\"' + '\", \"'.join(df_fuelprices['tech'].unique()) + '\")'\n",
    "c.execute(query)\n",
    "query = 'DELETE FROM CostVariable WHERE tech = \"E_ELCTDLOSS\"'\n",
    "c.execute(query)\n",
    "df_fuelprices.to_sql('CostVariable',conn, if_exists='append', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conn.commit()\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "con=sqlite3.connect(outputdB, isolation_level=None)\n",
    "con.execute(\"VACUUM\")\n",
    "con.commit()\n",
    "con.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conn = sqlite3.connect(outputdB)\n",
    "with open(outputdB.replace('ite',''), 'w') as f:\n",
    "    for line in conn.iterdump():\n",
    "        '''\n",
    "        if 'e+' in line or 'e-0' in line:\n",
    "            val = [x for x in line.split(',') if 'e+' in x or 'e-0' in x][0]\n",
    "            ls = line.split(',')\n",
    "            ind = ls.index(val)\n",
    "            ls[ind] = [str(round(float(x),2)) for x in line.split(',') if 'e+' in x or 'e-0' in x ][0]\n",
    "            line = ','.join(ls)\n",
    "        '''\n",
    "        if 'CREATE TABLE' in line:\n",
    "            f.write('\\n' )\n",
    "        f.write('%s\\n' % line)\n",
    "        \n",
    "conn.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
